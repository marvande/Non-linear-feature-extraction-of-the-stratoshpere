{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non linear feature extraction of the stratosphere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import warnings\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import scipy.sparse.linalg as linalg\n",
    "import scipy.signal as signal\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import os\n",
    "import math \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from functions_takens import *\n",
    "\n",
    "%load_ext jupyternotify\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of nearest neighbours: 0.1\n",
      "Number of eigenmaps: 21\n",
      "Data set considered: raw\n",
      "Path to input data: ../../../data/vandermeer/input_data/\n",
      "Using takens embedding: True\n",
      "tau = 2.0 months\n"
     ]
    }
   ],
   "source": [
    "# Percentage of nearest neighbours:\n",
    "PERC_NEIGH = 10\n",
    "print(f'Percentage of nearest neighbours: {PERC_NEIGH/100}')\n",
    "\n",
    "# Number of eigenmaps to compute:\n",
    "NUM_EIGENVALUES = 21\n",
    "print(f'Number of eigenmaps: {NUM_EIGENVALUES}')\n",
    "\n",
    "# Data set to consider: ('raw/anomalies')\n",
    "DATA = 'anomalies'\n",
    "print(f'Data set considered: {DATA}')\n",
    "\n",
    "# Path to input data:\n",
    "INPUT_DATA = '../../../data/vandermeer/input_data/'\n",
    "print(f'Path to input data: {INPUT_DATA}')\n",
    "\n",
    "# Wether to do NLSA or Laplacian Eigenmaps. Takens True for NLSA\n",
    "USE_TAKENS = True\n",
    "print(f'Using takens embedding: {USE_TAKENS}')\n",
    "\n",
    "# Time-step in Takens embedding:\n",
    "TAU = 4 * 30 * 2\n",
    "print(f'tau = {TAU/(4*30)} months')\n",
    "\n",
    "BEGIN_YEAR = 1979\n",
    "END_YEAR = 2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants for plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 14\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)  # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)  # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=BIGGER_SIZE)  # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Paths:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global path to save data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global path: ../../../data/vandermeer/pickles/raw/\n"
     ]
    }
   ],
   "source": [
    "if DATA == 'raw':\n",
    "    PATH = '../../../data/vandermeer/pickles/raw/'\n",
    "elif DATA == 'anomalies': \n",
    "    PATH = '../../../data/vandermeer/pickles/anomalies/'\n",
    "if not os.path.exists(PATH):\n",
    "        os.makedirs(PATH)\n",
    "print(f'Global path: {PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to save data according to the percentage of neighbours used: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precise path: ../../../data/vandermeer/pickles/raw/10perc/\n"
     ]
    }
   ],
   "source": [
    "PATH1 = PATH + str(PERC_NEIGH)+'perc/'\n",
    "if not os.path.exists(PATH1):\n",
    "        os.makedirs(PATH1)\n",
    "print(f'Precise path: {PATH1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to save data of simple kernel (binary kernel): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to simple kernel: ../../../data/vandermeer/pickles/raw/10perc/simple_kernel/\n",
      "Path to simple kernel for NLSA: ../../../data/vandermeer/pickles/raw/10perc/simple_kernel/takens/\n"
     ]
    }
   ],
   "source": [
    "# path to simple_kernel:\n",
    "PATH_SIMPLE = PATH1 + 'simple_kernel/'\n",
    "if not os.path.exists(PATH_SIMPLE):\n",
    "        os.makedirs(PATH_SIMPLE)\n",
    "print(f'Path to simple kernel: {PATH_SIMPLE}')\n",
    "\n",
    "PATH_SIMPLE_TAKENS = PATH1 + 'simple_kernel/takens/'\n",
    "if not os.path.exists(PATH_SIMPLE_TAKENS):\n",
    "        os.makedirs(PATH_SIMPLE_TAKENS)\n",
    "print(f'Path to simple kernel for NLSA: {PATH_SIMPLE_TAKENS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to save results of NLSA with correct neighbour percentage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to NLSA results:\n",
    "PATH_TAKENS = PATH + str(PERC_NEIGH)+'perc/takens/'\n",
    "if not os.path.exists(PATH_TAKENS):\n",
    "        os.makedirs(PATH_TAKENS)\n",
    "print(f'Path to NLSA results: {PATH_TAKENS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data:\n",
    "\n",
    "Load inut raw or anomalies data with basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "anomalies_cf = pd.read_csv(INPUT_DATA + 'anomalies_coefficients.csv', sep=',')\n",
    "raw_cf = pd.read_csv(INPUT_DATA + 'raw_data_coefficients.csv', sep=',')\n",
    "basis_cf = pd.read_csv(INPUT_DATA + 'basis_functions.csv', sep=',')\n",
    "\n",
    "if DATA == 'raw':\n",
    "    df = raw_cf\n",
    "elif DATA == 'anomalies':\n",
    "    df = anomalies_cf\n",
    "\n",
    "# remove useless axes:\n",
    "df = df.drop(['Unnamed: 0', 'Date'], axis=1)\n",
    "print('Sample of data:')\n",
    "print(f'Shape of {DATA} data: {df.shape}')\n",
    "pd.DataFrame(df).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplacian Eigenmaps:\n",
    "\n",
    "Laplacian eigenmap decomposition according to this [paper](https://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance matrix:\n",
    "Calculate matrix of distances between all points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to re-compute distance matrix:\n",
    "\"\"\"\n",
    "%%notify\n",
    "print(f'Distance matrix for {DATA}')\n",
    "distance_df = pd.DataFrame(distance_matrix(df.values, df.values),\n",
    "                           index=df.index,\n",
    "                           columns=df.index)\n",
    "distance_df.to_pickle(PATH + 'distance_matrix.pkl')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-computed distance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f'Distance matrix read from {PATH1}')\n",
    "distance_matrix = pd.read_pickle(PATH1+'distance_matrix.pkl').values\n",
    "\n",
    "#check distance matrix is symmetric:\n",
    "assert(np.all(distance_matrix.T == distance_matrix))\n",
    "\n",
    "print(f'Distance matrix shape: {distance_matrix.shape}')\n",
    "print('Sample of distance matrix:')\n",
    "pd.DataFrame(distance_matrix).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image of distance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taken's embedding:\n",
    "\n",
    "Calculate new distance matrix from the original one using Taken's embedding. In this new matrix \n",
    "\n",
    "$dist(Y(t_1), Y(t_2))=\\sqrt(\\sum_{j=0}^{\\tau-1}dist(X(t_1 +j),X(t_2+j))^2)$ \n",
    "\n",
    "Where $\\tau$ is the time-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "distance_matrix = pd.read_pickle(PATH1 + 'distance_matrix.pkl').values\n",
    "print(f'Distance matrix shape: {distance_matrix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA == 'raw':\n",
    "    df = pd.read_csv(INPUT_DATA + 'raw_data_coefficients.csv', sep=',')\n",
    "elif DATA == 'anomalies': \n",
    "    df = pd.read_csv(INPUT_DATA + 'anomalies_coefficients.csv', sep=',')\n",
    "print(f'Input data shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get time information and the number of measures per year: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = df['Date']\n",
    "time = pd.to_datetime(time)\n",
    "df_ = df.drop(['Unnamed: 0', 'Date'], axis=1)\n",
    "df_time = pd.concat([time, df_], axis=1)\n",
    "df_time = df_time.set_index('Date')\n",
    "\n",
    "years = range(BEGIN_YEAR, END_YEAR)\n",
    "years = [str(y) for y in years]\n",
    "\n",
    "counts = {}\n",
    "sum_ = 0\n",
    "for y in years:\n",
    "    sum_ += len(df_time[y])\n",
    "    counts[y] = len(df_time[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get positions of last measure per year so that we don't cross to another year when computing the Takens matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = [counts[i] for i in years]\n",
    "last_year_pos = [np.sum(cs[:i+1]) for i in range(len(cs))]\n",
    "\n",
    "entire_winters = END_YEAR-BEGIN_YEAR-1\n",
    "print(f'Number of entire winters: {entire_winters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get last and first positions of entire winters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_pos_winters = give_last_post_winters(last_year_pos, years, df_time)\n",
    "first_pos_winters = give_first_post_winters(last_year_pos, years, df_time)\n",
    "assert(len(last_pos_winters)==len(first_pos_winters))\n",
    "\n",
    "np.save(PATH1+'last_pos_winters.npy', last_pos_winters)\n",
    "np.save(PATH1+'first_pos_winters.npy', first_pos_winters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_of_points = []\n",
    "for i in range(len(first_pos_winters)):\n",
    "    indices_of_points.append(\n",
    "        range(first_pos_winters[i] + TAU, last_pos_winters[i] + 1, 1))\n",
    "for i in indices_of_points:\n",
    "    assert (i[-1] - i[0] > 600)\n",
    "\n",
    "indices_of_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Safety check to see everything is working all right:\n",
    "# total number of points:\n",
    "le = np.sum([len(i) for i in indices_of_points])\n",
    "# time series corresponding to those points:\n",
    "time_nlsa = time[indices_of_points[0]]\n",
    "for i in range(1, len(last_pos_winters)):\n",
    "    time_nlsa = pd.concat([time_nlsa, time[indices_of_points[i]]], axis = 0)\n",
    "assert(le==len(time_nlsa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unwrap the indices of all ranges of `indices_of_points` to get one list of all indices we need to go over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_m = []\n",
    "for j in range(len(indices_of_points)):\n",
    "    for i in range(len(indices_of_points[j])):\n",
    "        indices_m.append(indices_of_points[j][i])\n",
    "assert(len(indices_m)==le)\n",
    "print(f'First and last indice: {indices_m[0]}, {indices_m[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the new distance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Number of entire winters:\n",
    "num_years = END_YEAR-BEGIN_YEAR-1\n",
    "print(f'Number of years: {num_years}')\n",
    "\n",
    "m,n  = le, le\n",
    "dist_Y = np.zeros((m,n))\n",
    "print(f'New distance matrix shape: {dist_Y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"%%time\n",
    "dist_Y = apply_takens(dist_Y, distance_matrix, indices_m, TAU, PATH1)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now just upper triangle matrix so make it symmetric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f'Reading upper triangle matrix at {PATH1}:')\n",
    "dist_Y = pd.read_pickle(PATH1+'dist_Y_takens_final.pkl').values\n",
    "\n",
    "# Make matrix symetric:\n",
    "takens_matrix = dist_Y + dist_Y.T\n",
    "\n",
    "# Save NLSA distance matrix matrix:\n",
    "print(f'Save NLSA distance matrix at {PATH1}')\n",
    "takens_df = pd.DataFrame(takens_matrix)\n",
    "takens_df.to_pickle(PATH1 + 'distance_matrix_takens_final.pkl')\n",
    "\n",
    "del dist_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read NLSA distance matrix from memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Reading Takens matrix:')\n",
    "takens_matrix = pd.read_pickle(PATH1 + 'distance_matrix_takens.pkl').values\n",
    "\n",
    "#check distance matrix is symmetric:\n",
    "assert(np.all(takens_matrix.T == takens_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "takens_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(takens_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(takens_matrix)\n",
    "del takens_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbours:\n",
    "Calculate matrix with neighbouring vertices, e.g. 1 for $x_j,x_i$ if $x_j$ or $x_i$ selected the other as a closest  neighbour and 0 otherwise. This weight matrix will also serve as the simple kernel. Creates the binary weight matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Percentage of neighbours\n",
    "perc = PERC_NEIGH / 100\n",
    "print(f'Starting nearest neighbours for {perc*100}% nearest neihbours ')\n",
    "\n",
    "# Reading input data:\n",
    "if DATA == 'raw':\n",
    "    print(f'Reading raw input data from: {INPUT_DATA}')\n",
    "    df = pd.read_csv(INPUT_DATA + 'raw_data_coefficients.csv', sep=',')\n",
    "elif DATA == 'anomalies': \n",
    "    print(f'Reading anomalies input data from: {INPUT_DATA}')\n",
    "    df = pd.read_csv(INPUT_DATA + 'anomalies_coefficients.csv', sep=',')\n",
    "\n",
    "print(f'Reading distance matrix')\n",
    "\n",
    "if USE_TAKENS == True:\n",
    "    print(f'Using takens embedding, reading from {PATH_SIMPLE}')\n",
    "    distance_matrix = pd.read_pickle(PATH1 + 'distance_matrix_takens.pkl').values\n",
    "    print(f'Distance matrix shape: {distance_matrix.shape}')\n",
    "else:\n",
    "    print(f'Using normal distance matrix, reading from {PATH1}')\n",
    "    distance_matrix = pd.read_pickle(PATH1+'distance_matrix.pkl').values\n",
    "    print(f'Distance matrix shape: {distance_matrix.shape}')\n",
    "\n",
    "# K-nearest neighbours:\n",
    "print(f'Look for {perc*100}% nearest neihbours:')\n",
    "K = int(len(df.values) * perc)\n",
    "N = len(distance_matrix)\n",
    "weight_matrix = np.zeros((N, N))\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    # select K closest neihbours:\n",
    "    indices = np.argsort(distance_matrix[i])[:K]\n",
    "    for j in indices:\n",
    "        if i != j and weight_matrix[i, j] == 0:\n",
    "            weight_matrix[i, j] += 1\n",
    "weight_matrix = weight_matrix.T\n",
    "\n",
    "# Make weight matrix is symmetric:\n",
    "for i in tqdm(range(N)):\n",
    "    indices = np.argsort(distance_matrix.T[i])[:K]\n",
    "    for j in indices:\n",
    "        if i != j and weight_matrix[i, j] == 0:\n",
    "            weight_matrix[i, j] += 1\n",
    "weight_df = pd.DataFrame(weight_matrix)\n",
    "\n",
    "# Save weight matrix:\n",
    "if USE_TAKENS:\n",
    "    PATH_SAVE = PATH_SIMPLE_TAKENS+'weight_matrix_takens_{}.pkl'.format(PERC_NEIGH)\n",
    "    print('Saving weight matrix at {}'.format(PATH_SAVE))\n",
    "    weight_df.to_pickle(PATH_SAVE)\n",
    "else:\n",
    "    PATH_SAVE = PATH_SIMPLE+'weight_matrix_{}.pkl'.format(PERC_NEIGH)\n",
    "    print('Saving weight matrix at {}'.format(PATH_SAVE))\n",
    "    weight_df.to_pickle(PATH_SAVE)\n",
    "\n",
    "del weight_df, weight_matrix, distance_matrix\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read weight matrix:\n",
    "if USE_TAKENS:\n",
    "    print(f'Reading binary weight matrix from {PATH_SIMPLE_TAKENS}')\n",
    "    weight_m = pd.read_pickle(PATH_SIMPLE_TAKENS +\n",
    "                          'weight_matrix_takens_{}.pkl'.format(PERC_NEIGH)).values\n",
    "else:\n",
    "    print(f'Reading binary weight matrix from {PATH_SIMPLE}')\n",
    "    weight_m = pd.read_pickle(PATH_SIMPLE +\n",
    "                          'weight_matrix_{}.pkl'.format(PERC_NEIGH)).values\n",
    "# Test if matrix is symetric:\n",
    "assert (np.all(weight_m.T == weight_m))\n",
    "print(f'Weight matrix shape: {weight_m.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(weight_m).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at image of weight matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(weight_m)\n",
    "del weight_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary kernel: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Laplacian eigenmaps for the binary kernel (0/1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagonal matrix:\n",
    "Diagonal matrix from binary weight matrix, computed as $D_{ij} = \\sum_{j}W_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if USE_TAKENS:\n",
    "    weight_m = pd.read_pickle(PATH_SIMPLE_TAKENS +\n",
    "                          'weight_matrix_takens_{}.pkl'.format(PERC_NEIGH)).values\n",
    "else:\n",
    "    weight_m = pd.read_pickle(PATH_SIMPLE +\n",
    "                          'weight_matrix_{}.pkl'.format(PERC_NEIGH)).values\n",
    "\n",
    "# Compute diagonal matrix:\n",
    "D = np.zeros((len(weight_m), len(weight_m)))\n",
    "print('Computing diagonal matrix:')\n",
    "for i in tqdm(range(len(weight_m))):\n",
    "    D[i, i] = np.sum(weight_m[i])\n",
    "D_df = pd.DataFrame(D)\n",
    "\n",
    "\n",
    "if USE_TAKENS:\n",
    "    print(f'Saving diagonal matrix at: {PATH_SIMPLE_TAKENS}')\n",
    "    D_df.to_pickle(PATH_SIMPLE_TAKENS+'diagonal_matrix_takens_{}.pkl'.format(PERC_NEIGH))\n",
    "else:\n",
    "    print(f'Saving diagonal matrix at: {PATH_SIMPLE}')\n",
    "    D_df.to_pickle(PATH_SIMPLE+'diagonal_matrix_{}.pkl'.format(PERC_NEIGH))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplacian matrix:\n",
    "\n",
    "Laplacian matrix computed as $L = D-W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Computing Laplacian matrix:')\n",
    "\n",
    "L = np.subtract(D, weight_m)\n",
    "L_df = pd.DataFrame(L)\n",
    "\n",
    "if USE_TAKENS: \n",
    "    print(f'Saving laplacian matrix at: {PATH_SIMPLE_TAKENS}')\n",
    "    L_df.to_pickle(PATH_SIMPLE_TAKENS + 'laplacian_simple_matrix_takens_{}.pkl'.format(PERC_NEIGH))\n",
    "else:\n",
    "    print(f'Saving laplacian matrix at: {PATH_SIMPLE}')\n",
    "    L_df.to_pickle(PATH_SIMPLE + 'laplacian_simple_matrix_{}.pkl'.format(PERC_NEIGH))\n",
    "\n",
    "del L, L_df, D, weight_m\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvalues:\n",
    "Eigenvalues and eigenvectors for binary kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if USE_TAKENS:\n",
    "    print(f'Reading D and L from: {PATH_SIMPLE_TAKENS}')\n",
    "    D = pd.read_pickle(\n",
    "        PATH_SIMPLE_TAKENS +\n",
    "        'diagonal_matrix_takens_{}.pkl'.format(PERC_NEIGH)).values\n",
    "    print(f'Diagonal matrix shape: {D.shape}')\n",
    "    L = pd.read_pickle(\n",
    "        PATH_SIMPLE_TAKENS +\n",
    "        'laplacian_simple_matrix_takens_{}.pkl'.format(PERC_NEIGH)).values\n",
    "    print(f'Laplacian matrix shape: {L.shape}')\n",
    "\n",
    "else:\n",
    "    print(f'Reading D and L from: {PATH_SIMPLE}')\n",
    "    D = pd.read_pickle(PATH_SIMPLE +\n",
    "                       'diagonal_matrix_{}.pkl'.format(PERC_NEIGH)).values\n",
    "    L = pd.read_pickle(\n",
    "        PATH_SIMPLE +\n",
    "        'laplacian_simple_matrix_{}.pkl'.format(PERC_NEIGH)).values\n",
    "\n",
    "# Compute first eigenvalues\n",
    "print(f'Computing {NUM_EIGENVALUES} eigenvalues:')\n",
    "w, eigv = linalg.eigs(L, k=NUM_EIGENVALUES, M=D, which='SM')\n",
    "\n",
    "# Save values:\n",
    "if USE_TAKENS:\n",
    "    print(f'Saving eigenvalues and eigenvectors to: {PATH_SIMPLE_TAKENS}')\n",
    "    pd.DataFrame(w).to_pickle(PATH_SIMPLE_TAKENS +\n",
    "                              'eigenvalues_takens_{}.pkl'.format(PERC_NEIGH))\n",
    "    pd.DataFrame(eigv).to_pickle(\n",
    "        PATH_SIMPLE_TAKENS + 'eigenvectors_takens_{}.pkl'.format(PERC_NEIGH))\n",
    "    print(f'Eigenvector shape: {eigv.shape}')\n",
    "else:\n",
    "    print(f'Saving eigenvalues and eigenvectors to: {PATH_SIMPLE}')\n",
    "    pd.DataFrame(w).to_pickle(PATH_SIMPLE +\n",
    "                              'eigenvalues_{}.pkl'.format(PERC_NEIGH))\n",
    "    pd.DataFrame(eigv).to_pickle(PATH_SIMPLE +\n",
    "                                 'eigenvectors_{}.pkl'.format(PERC_NEIGH))\n",
    "del L, D, w, eigv\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat kernel: \n",
    "The heat kernel is computed as $W_{ij} = exp(\\frac{-||x_j-x_i||^2_2}{t})$ for connected neighbours and 0 otherwise.\n",
    "Source:[paper](https://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of bandwidth: \n",
    "Compute the right bandwidth $t$ values for the heat kernel. Try the mean, median and maximum over distances of neihgbouring vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f'Reading distance and weight matrix from: {PATH1}')\n",
    "\n",
    "if USE_TAKENS: \n",
    "    print('Using takens:')\n",
    "    distance_matrix = pd.read_pickle(PATH1 + 'distance_matrix_takens.pkl').values\n",
    "    weight_m = pd.read_pickle(PATH_SIMPLE_TAKENS +'weight_matrix_takens_{}.pkl'.format(PERC_NEIGH)).values\n",
    "else:\n",
    "    distance_matrix = pd.read_pickle(PATH1+'distance_matrix.pkl').values\n",
    "    weight_m = pd.read_pickle(PATH_SIMPLE +'weight_matrix_{}.pkl'.format(PERC_NEIGH)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Select only distances that are chosen in the nearest neighbours step:\n",
    "mult_df = np.multiply(distance_matrix, weight_m)\n",
    "non_zero_mult = np.extract(mult_df > 0, mult_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean, median and max over non-zero distances:\n",
    "mean_distances = np.mean(non_zero_mult)\n",
    "median_dist = np.median(non_zero_mult)\n",
    "max_distances = np.max(non_zero_mult)\n",
    "\n",
    "t = [mean_distances, median_dist, max_distances]\n",
    "\n",
    "ts = {\n",
    "    'mean_distances': mean_distances,\n",
    "    'median_dist': median_dist,\n",
    "    'max_distances': max_distances,\n",
    "    'mean_dist_all': np.mean(distance_matrix),\n",
    "    'max_dist_all': np.max(distance_matrix),\n",
    "}\n",
    "if USE_TAKENS:\n",
    "    np.save(PATH1 + 't_takens_.npy', np.array(t))\n",
    "else:\n",
    "    np.save(PATH1+'t.npy', np.array(t))\n",
    "print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,5))\n",
    "axs = plt.subplot(1,1,1)\n",
    "axs.hist(non_zero_mult)\n",
    "#plt.title('Non zero distances between neighbours:')\n",
    "axs.axvline(t[0], color = 'green', label = 'mean')\n",
    "axs.axvline(t[1], color = 'orange', label = 'median')\n",
    "axs.axvline(t[2], color = 'red', label = 'max')\n",
    "axs.set_xlabel('Distance between neihbours')\n",
    "axs.set_ylabel('Count')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del distance_matrix, weight_m, mult_df, non_zero_mult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heat matrix: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing heat kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if USE_TAKENS:\n",
    "    t = np.load(PATH1+'t_takens_.npy')\n",
    "    chosen_t = t[0]\n",
    "    corr_t = ['mean']\n",
    "    \n",
    "    print(f'Reading distance and weight matrix from: {PATH_SIMPLE}')\n",
    "    distance_matrix = pd.read_pickle(PATH1 + 'distance_matrix_takens.pkl').values\n",
    "    weight_m = pd.read_pickle(PATH_SIMPLE_TAKENS+'weight_matrix_takens_{}.pkl'.format(PERC_NEIGH)).values\n",
    "    \n",
    "    print('Computing heat matrix for bandwitdh {}: {}'.format(corr_t[0],chosen_t))\n",
    "    PATH2 = PATH1+'t_'+corr_t[0]+'/takens/'\n",
    "    if not os.path.exists(PATH2):\n",
    "            os.makedirs(PATH2)\n",
    "\n",
    "    distance_df = pd.DataFrame(distance_matrix)\n",
    "    # Create heat matrix:\n",
    "    heat_matrix_df = distance_df.apply(lambda x: np.exp(-(x**2) / (chosen_t**2)))\n",
    "    heat_matrix_df = pd.DataFrame(np.multiply(weight_m, heat_matrix_df))\n",
    "    print(f'Saving heat matrix at {PATH2}')\n",
    "    heat_matrix_df.to_pickle(PATH2+'heat_matrix_'+'t_'+corr_t[0]+ '_takens_.pkl')\n",
    "    \n",
    "else:\n",
    "    t = np.load(PATH1+'t.npy')\n",
    "    corr_t = ['mean', 'med', 'max']\n",
    "    \n",
    "    print(f'Reading distance and weight matrix from: {PATH1}')\n",
    "    distance_matrix = pd.read_pickle(PATH1+'distance_matrix.pkl').values\n",
    "    weight_m = pd.read_pickle(PATH_SIMPLE+'weight_matrix_{}.pkl'.format(PERC_NEIGH)).values\n",
    "    \n",
    "    for i in range(3):\n",
    "        chosen_t = t[i]\n",
    "        print('Computing heat matrix for bandwitdh {}: {}'.format(corr_t[i],chosen_t))\n",
    "\n",
    "        PATH2 = PATH1+'t_'+corr_t[i]+'/'\n",
    "        if not os.path.exists(PATH2):\n",
    "            os.makedirs(PATH2)\n",
    "\n",
    "        distance_df = pd.DataFrame(distance_matrix)\n",
    "        # Create heat matrix:\n",
    "        heat_matrix_df = distance_df.apply(lambda x: np.exp(-(x**2) / (chosen_t**2)))\n",
    "        heat_matrix_df = pd.DataFrame(np.multiply(weight_m, heat_matrix_df))\n",
    "        heat_matrix_df.to_pickle(PATH2+'heat_matrix_'+'t_'+corr_t[i]+ '_.pkl')\n",
    "del heat_matrix_df, distance_matrix, weight_m, distance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-computed heat kernels for 10% neighbours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corr_t = ['mean', 'med', 'max']\n",
    "\n",
    "\n",
    "if USE_TAKENS:\n",
    "    PATH_READ = '../../../data/vandermeer/pickles/{}/10perc/'.format(DATA)\n",
    "    print(f'Path to heat matrices:{PATH_READ}')\n",
    "    print(f'Reading heat matrix for t = {corr_t[0]}:')\n",
    "    heat_matrix_mean = pd.read_pickle(PATH_READ +'t_'+corr_t[0]+ '/takens/' +\n",
    "                                      'heat_matrix_' +'t_'+corr_t[0]+'_takens_.pkl').values\n",
    "    print(f'Heat matrix shape: {heat_matrix_mean.shape}')\n",
    "else:\n",
    "    PATH_READ = '../../../data/vandermeer/pickles/{}/10perc/'.format(DATA)\n",
    "    print(f'Path to heat matrices:{PATH_READ}')\n",
    "    print(f'Reading heat matrix for t = {corr_t[0]}:')\n",
    "    heat_matrix_mean = pd.read_pickle(PATH_READ +'t_'+corr_t[0]+ '/' +\n",
    "                                      'heat_matrix_' +'t_'+corr_t[0]+'_.pkl').values\n",
    "    print(f'Reading heat matrix for t = {corr_t[1]}:')\n",
    "    heat_matrix_max = pd.read_pickle(PATH_READ +'t_'+corr_t[1]+ '/' +\n",
    "                                     'heat_matrix_' +'t_'+corr_t[1]+'_.pkl').values\n",
    "    print(f'Reading heat matrix for t = {corr_t[2]}:')\n",
    "    heat_matrix_med = pd.read_pickle(PATH_READ +'t_'+corr_t[2]+ '/' +\n",
    "                                     'heat_matrix_' +'t_'+corr_t[2]+'_.pkl').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-computed heat kernels for 20% neighbours if you want to compare to those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not USE_TAKENS:\n",
    "    corr_t = ['mean', 'med', 'max']\n",
    "    PATH_READ = '../../../data/vandermeer/pickles/{}/20perc/'.format(DATA)\n",
    "    print(f'Path to heat matrices:{PATH_READ}')\n",
    "    print(f'Reading heat matrix for t = {corr_t[0]}:')\n",
    "    heat_matrix_mean2 = pd.read_pickle(PATH_READ +'t_'+corr_t[0]+ '/' +\n",
    "                                      'heat_matrix_' +'t_'+corr_t[0]+\n",
    "                                      '_.pkl').values\n",
    "    print(f'Reading heat matrix for t = {corr_t[1]}:')\n",
    "    heat_matrix_max2 = pd.read_pickle(PATH_READ +'t_'+corr_t[1]+ '/' +\n",
    "                                     'heat_matrix_' +'t_'+corr_t[1]+\n",
    "                                     '_.pkl').values\n",
    "    print(f'Reading heat matrix for t = {corr_t[2]}:')\n",
    "    heat_matrix_med2 = pd.read_pickle(PATH_READ +'t_'+corr_t[2]+ '/' +\n",
    "                                     'heat_matrix_' +'t_'+corr_t[2]+\n",
    "                                     '_.pkl').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot sample of heat kernels as image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not USE_TAKENS:\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 10))\n",
    "    i = 0\n",
    "    corr_t = ['mean', 'med', 'max']\n",
    "    matrices = [heat_matrix_mean[:10, :10],heat_matrix_max[:10, :10],heat_matrix_med[:10, :10]]\n",
    "    for chosen_t in corr_t:\n",
    "        axs[i].imshow(matrices[i])\n",
    "        axs[i].set_title(chosen_t)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histogram of non zero values of heat kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if USE_TAKENS:\n",
    "    fig, axs = plt.subplots(1,1, figsize = (5,5))\n",
    "    corr_t = ['mean']\n",
    "    non_zero_heat = np.extract(heat_matrix_mean>0, heat_matrix_mean)\n",
    "    axs.set_title('Heat kernel, t = {} distance'.format(corr_t[0]))\n",
    "    axs.hist(non_zero_heat, label = '10 perc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if not USE_TAKENS:\n",
    "    fig, axs = plt.subplots(1,3, figsize = (15,5))\n",
    "    i = 0\n",
    "    corr_t = ['mean', 'med', 'max']\n",
    "    matrices = [heat_matrix_mean, heat_matrix_max, heat_matrix_med]\n",
    "    for i in range(3):\n",
    "        heat_matrix = matrices[i]\n",
    "        non_zero_heat = np.extract(heat_matrix>0, heat_matrix)\n",
    "        axs[i].set_title('Heat kernel, t = {} distance'.format(corr_t[i]))\n",
    "        axs[i].hist(non_zero_heat, label = '10 perc')\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot overlapping histogram of non zero values of heat kernels for 10% and 20%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not USE_TAKENS:\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    i = 0\n",
    "    matrices = [heat_matrix_mean, heat_matrix_max, heat_matrix_med]\n",
    "    matrices2 = [heat_matrix_mean2, heat_matrix_max2, heat_matrix_med2]\n",
    "    corr_t = ['mean', 'med', 'max']\n",
    "    for i in range(3):\n",
    "        heat_matrix = matrices[i]\n",
    "        heat_matrix2 = matrices2[i]\n",
    "        non_zero_heat = np.extract(heat_matrix > 0, heat_matrix)\n",
    "        non_zero_heat2 = np.extract(heat_matrix2 > 0, heat_matrix2)\n",
    "        t = ['mean', 'max', 'median']\n",
    "        axs[i].set_title('Heat kernel, t = {} distance'.format(corr_t[i]))\n",
    "        axs[i].hist(non_zero_heat, label='10 perc', alpha=0.5)\n",
    "        axs[i].hist(non_zero_heat2, label='20 perc', alpha=0.5)\n",
    "        i += 1\n",
    "    plt.legend()\n",
    "    del matrices, matrices2, heat_matrix_mean, heat_matrix_max, heat_matrix_med, heat_matrix_mean2, heat_matrix_max2, heat_matrix_med2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagonal weight matrix: \n",
    "Compute diagonal matrix as $D_{ii} = \\sum_j W_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TAKENS:\n",
    "    corr_t = ['mean']\n",
    "    print(f'Computing diagonal matrix for heat kernel with t: {corr_t[0]}')\n",
    "    PATH2 = PATH1+'t_'+corr_t[0]+'/takens/'\n",
    "    if not os.path.exists(PATH2):\n",
    "        os.makedirs(PATH2)\n",
    "    wm = pd.read_pickle(PATH1+'t_'+corr_t[0]+'/takens/heat_matrix_t_'+corr_t[0]+'_takens_.pkl').values\n",
    "    D = np.zeros((len(wm),len(wm)))\n",
    "    for i in tqdm(range(len(wm))):\n",
    "        D[i,i] = np.sum(wm[i])\n",
    "    D_df = pd.DataFrame(D)\n",
    "    print(f'Writing diagonal matrix to {PATH2}')\n",
    "    D_df.to_pickle(PATH2+'diagonal_heat_matrix_t_'+corr_t[0]+'_takens_.pkl')\n",
    "else:\n",
    "    corr_t = ['mean', 'med', 'max']\n",
    "    for j in range(3):\n",
    "        print(f'Computing diagonal matrix for heat kernel with t: {corr_t[j]}')\n",
    "        PATH2 = PATH1+'t_'+corr_t[j]+'/'\n",
    "        if not os.path.exists(PATH2):\n",
    "            os.makedirs(PATH2)\n",
    "        wm = pd.read_pickle(PATH1+'t_'+corr_t[j]+'/heat_matrix_t_'+corr_t[j]+'_.pkl').values\n",
    "        D = np.zeros((len(wm),len(wm)))\n",
    "        for i in tqdm(range(len(wm))):\n",
    "            D[i,i] = np.sum(wm[i])\n",
    "        D_df = pd.DataFrame(D)\n",
    "        print(f'Writing diagonal matrix to {PATH2}')\n",
    "        D_df.to_pickle(PATH2+'diagonal_heat_matrix_t_'+corr_t[j]+'_.pkl')\n",
    "del D_df, D, wm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-computed diagonal matrix_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if USE_TAKENS:\n",
    "    D = pd.read_pickle(PATH1+'t_mean/takens/diagonal_heat_matrix_'+'t_mean_takens_.pkl').values\n",
    "else:\n",
    "    D = pd.read_pickle(PATH1+'t_mean/diagonal_heat_matrix_'+'t_mean_.pkl').values\n",
    "print(f'Shape of diagonal matrix: {D.shape}')\n",
    "pd.DataFrame(D).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplacian matrix: \n",
    "\n",
    "Compute Laplacian matrix as $L = D- W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if USE_TAKENS:\n",
    "    corr_t = ['mean']\n",
    "    print(f'Computing Laplacian matrix for heat kernel with t: {corr_t[0]}')\n",
    "    PATH2 = PATH1 + 't_' + corr_t[0] + '/takens/'\n",
    "    if not os.path.exists(PATH2):\n",
    "        os.makedirs(PATH2)\n",
    "    print('Reading heat matrix:')\n",
    "    wm = pd.read_pickle(PATH2+'heat_matrix_t_'+corr_t[0]+'_takens_.pkl').values\n",
    "    print('Reading diagonal matrix:')\n",
    "    D = pd.read_pickle(PATH2+'diagonal_heat_matrix_t_'+corr_t[0]+'_takens_.pkl').values\n",
    "    print('Calculating Laplacian:')\n",
    "    L = np.subtract(D, wm)\n",
    "    L_df = pd.DataFrame(L)\n",
    "    print(f'Writing Laplacian to {PATH2}')\n",
    "    L_df.to_pickle(PATH2+'laplacian_heat_matrix_t_'+corr_t[0]+'_takens_.pkl')\n",
    "\n",
    "else:\n",
    "    corr_t = ['mean', 'med', 'max']\n",
    "    for j in range(3):\n",
    "        print(f'Computing Laplacian matrix for heat kernel with t: {corr_t[j]}')\n",
    "        PATH2 = PATH1 + 't_' + corr_t[j] + '/'\n",
    "        if not os.path.exists(PATH2):\n",
    "            os.makedirs(PATH2)\n",
    "        print('Reading heat matrix:')\n",
    "        wm = pd.read_pickle(PATH2+'heat_matrix_t_'+corr_t[j]+'_.pkl').values\n",
    "        print('Reading diagonal matrix:')\n",
    "        D = pd.read_pickle(PATH2+'diagonal_heat_matrix_t_'+corr_t[j]+'_.pkl').values\n",
    "        print('Calculating Laplacian:')\n",
    "        L = np.subtract(D, wm)\n",
    "        L_df = pd.DataFrame(L)\n",
    "        L_df.to_pickle(PATH2+'laplacian_heat_matrix_t_'+corr_t[j]+'_.pkl')\n",
    "    del L_df, L, D, wm\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-computed laplacian matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if USE_TAKENS:\n",
    "    D = pd.read_pickle(PATH1+'t_mean/takens/laplacian_heat_matrix_t_mean_takens_.pkl').values\n",
    "else:\n",
    "    D = pd.read_pickle(PATH1+'t_mean/laplacian_heat_matrix_t_mean_.pkl').values\n",
    "print(f'Shape of Laplacian matrix: {L.shape}')\n",
    "pd.DataFrame(L).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvalues: \n",
    "\n",
    "Eigendecomposition of: $Lf = \\gamma Df$ where $f$ are the eigenvector solutions ordered according to their increasing eigenvalue $\\lambda_0 = 0 < \\lambda_1 < ...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing eigendecomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chosen_t = 'mean'\n",
    "print(f'Computing eigenvalues for heat kernel with t={chosen_t}')\n",
    "\n",
    "if not os.path.exists(PATH2):os.makedirs(PATH2)\n",
    "\n",
    "if USE_TAKENS:  \n",
    "    PATH2 = PATH1+'t_{}/takens/'.format(chosen_t)\n",
    "    print(f'Loading L from:{PATH2}')   \n",
    "    L = pd.read_pickle(PATH2+'laplacian_heat_matrix_t_'+chosen_t+'_takens_.pkl').values\n",
    "\n",
    "    print(f'Loading D from:{PATH2}')   \n",
    "    D = pd.read_pickle(PATH2+'diagonal_heat_matrix_t_'+chosen_t+'_takens_.pkl').values\n",
    "\n",
    "    print(f'Computing {NUM_EIGENVALUES} eigenvalues:')\n",
    "    w, eigv = linalg.eigs(L, k=NUM_EIGENVALUES, M=D, which='SM')\n",
    "\n",
    "    print('Saving eigendecomposition:')\n",
    "    pd.DataFrame(w).to_pickle(PATH2 + 'eigenvalues_heat_matrix_t_'+chosen_t+'_takens_.pkl')\n",
    "    pd.DataFrame(eigv).to_pickle(PATH2 + 'eigenvectors_heat_matrix_t_'+chosen_t+ '_takens_.pkl')\n",
    "    print(f'Shape of eigenvectors: {eigv.shape}')\n",
    "else:\n",
    "    PATH2 = PATH1+'t_{}/'.format(chosen_t)\n",
    "    print(f'Loading L from:{PATH2}')   \n",
    "    L = pd.read_pickle(PATH2+'laplacian_heat_matrix_t_'+chosen_t+'_.pkl').values\n",
    "\n",
    "    print(f'Loading D from:{PATH2}')   \n",
    "    D = pd.read_pickle(PATH2+'diagonal_heat_matrix_t_'+chosen_t+'_.pkl').values\n",
    "\n",
    "    print(f'Computing {NUM_EIGENVALUES} eigenvalues:')\n",
    "    w, eigv = linalg.eigs(L, k=NUM_EIGENVALUES, M=D, which='SM')\n",
    "\n",
    "    print('Saving eigendecomposition:')\n",
    "    pd.DataFrame(w).to_pickle(PATH2 + 'eigenvalues_heat_matrix_t_'+chosen_t+'_.pkl')\n",
    "    pd.DataFrame(eigv).to_pickle(PATH2 + 'eigenvectors_heat_matrix_t_'+chosen_t+ '_.pkl')\n",
    "\n",
    "del L, D, w, eigv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create time-series:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading eigendecomposition for laplacian eigenmaps and NLSA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEAT KERNEL:\n",
    "chosen_t = 'mean'\n",
    "print(f'Loading eigendecomposition for t={chosen_t} at {PATH1}')\n",
    "\n",
    "w_mean_nlsa = pd.read_pickle(PATH1 + 't_' + chosen_t +\n",
    "                             '/takens/eigenvalues_heat_matrix_t_' + chosen_t +\n",
    "                             '_takens_.pkl')\n",
    "eigv_mean_nlsa = pd.read_pickle(PATH1 + 't_' + chosen_t +\n",
    "                                '/takens/eigenvectors_heat_matrix_t_' +\n",
    "                                chosen_t + '_takens_.pkl')\n",
    "\n",
    "w_mean = pd.read_pickle(PATH1 + 't_' + chosen_t +\n",
    "                        '/eigenvalues_heat_matrix_t_' + chosen_t + '_.pkl')\n",
    "eigv_mean = pd.read_pickle(PATH1 + 't_' + chosen_t +\n",
    "                           '/eigenvectors_heat_matrix_t_' + chosen_t + '_.pkl')\n",
    "\n",
    "# SIMPLE KERNEL:\n",
    "print(f'Loading eigendecomposition for simple kernel at {PATH_SIMPLE}')\n",
    "\n",
    "w_simple_nlsa = pd.read_pickle(PATH_SIMPLE_TAKENS +\n",
    "                               'eigenvalues_takens_{}.pkl'.format(PERC_NEIGH))\n",
    "eigv_simple_nlsa = pd.read_pickle(\n",
    "    PATH_SIMPLE_TAKENS + 'eigenvectors_takens_{}.pkl'.format(PERC_NEIGH))\n",
    "\n",
    "w_simple = pd.read_pickle(PATH_SIMPLE +\n",
    "                          'eigenvalues_{}.pkl'.format(PERC_NEIGH))\n",
    "eigv_simple = pd.read_pickle(PATH_SIMPLE +\n",
    "                             'eigenvectors_{}.pkl'.format(PERC_NEIGH))\n",
    "# Safety check:\n",
    "assert (eigv_simple_nlsa.shape == eigv_mean_nlsa.shape)\n",
    "assert (eigv_mean.shape == eigv_simple.shape)\n",
    "\n",
    "print(f'Shape of eigenvectors for NLSA: {eigv_mean_nlsa.shape}')\n",
    "print(f'Shape of eigenvectors for eigenmaps: {eigv_mean.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invert eigenvector 1 and 0 for raw data, weird bug, eigenvector that is nul is in second position for raw data so need to remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weirdly 0 in position 1 so invert position 0 and 1 in eigenvalues:\n",
    "if DATA == 'raw':\n",
    "    w_mean[0][1] = w_mean[0][0]\n",
    "    w_mean[0][0] = 0\n",
    "    w_mean_nlsa[0][1] = w_mean_nlsa[0][0]\n",
    "    w_mean_nlsa[0][0] = 0\n",
    "\n",
    "    w_simple[0][1] = w_simple[0][0]\n",
    "    w_simple[0][0] = 0\n",
    "    w_simple_nlsa[0][1] = w_simple_nlsa[0][0]\n",
    "    w_simple_nlsa[0][0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get time component for time-series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if DATA == 'anomalies':\n",
    "    df = pd.read_csv(INPUT_DATA + 'anomalies_coefficients.csv', sep=',')\n",
    "else:\n",
    "    df = pd.read_csv(INPUT_DATA + 'raw_data_coefficients.csv', sep=',')\n",
    "time = pd.to_datetime(df['Date'])\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create time-series from Laplacian eigenmaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigv_time_mean = pd.concat([time, pd.DataFrame(eigv_mean)], axis=1)\n",
    "eigv_time_mean = eigv_time_mean.set_index('Date')\n",
    "eigv_time_simple = pd.concat([time, pd.DataFrame(eigv_simple)], axis=1)\n",
    "eigv_time_simple = eigv_time_simple.set_index('Date')\n",
    "\n",
    "# drop eigenvector that is null:\n",
    "if DATA == 'raw':\n",
    "    eigv_time_mean = eigv_time_mean.drop([1], axis=1)\n",
    "    eigv_time_simple = eigv_time_simple.drop([1], axis=1)\n",
    "else:\n",
    "    eigv_time_mean = eigv_time_mean.drop([0], axis=1)\n",
    "    eigv_time_simple = eigv_time_simple.drop([0], axis=1)\n",
    "\n",
    "eigv_time_mean.columns = range(1, NUM_EIGENVALUES)\n",
    "eigv_time_simple.columns = range(1, NUM_EIGENVALUES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create time-series for NLSA eigenmaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pos_winters = np.load(PATH1 + 'first_pos_winters.npy')\n",
    "last_pos_winters = np.load(PATH1 + 'last_pos_winters.npy')\n",
    "\n",
    "indices_of_points = []\n",
    "for i in range(len(first_pos_winters)):\n",
    "    indices_of_points.append(\n",
    "        range(first_pos_winters[i] + TAU, last_pos_winters[i] + 1, 1))\n",
    "for i in indices_of_points:\n",
    "    assert (i[-1] - i[0] > 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_nlsa = time[indices_of_points[0]]\n",
    "\n",
    "for i in range(1, len(indices_of_points)):\n",
    "    time_nlsa = pd.concat([time_nlsa, time[indices_of_points[i]]], axis=0)\n",
    "\n",
    "assert (len(time_nlsa) == eigv_mean_nlsa.shape[0])\n",
    "\n",
    "time_nlsa = pd.to_datetime(time_nlsa)\n",
    "time_nlsa.to_csv(PATH_TAKENS + 'time_nlsa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigv_time_mean_nlsa = pd.DataFrame(eigv_mean_nlsa)\n",
    "eigv_time_mean_nlsa.index = time_nlsa\n",
    "eigv_time_simple_nlsa = pd.DataFrame(eigv_simple_nlsa)\n",
    "eigv_time_simple_nlsa.index = time_nlsa\n",
    "\n",
    "if DATA == 'raw':\n",
    "    eigv_time_simple_nlsa = eigv_time_simple_nlsa.drop([1], axis=1)\n",
    "    eigv_time_mean_nlsa = eigv_time_mean_nlsa.drop([1], axis=1)\n",
    "else:\n",
    "    eigv_time_simple_nlsa = eigv_time_simple_nlsa.drop([0], axis=1)\n",
    "    eigv_time_mean_nlsa = eigv_time_mean_nlsa.drop([0], axis=1)\n",
    "\n",
    "eigv_time_simple_nlsa.columns = range(1, NUM_EIGENVALUES)\n",
    "eigv_time_mean_nlsa.columns = range(1, NUM_EIGENVALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winters = {\n",
    "    1998: pd.date_range('1998-12', '1999-04'),\n",
    "    2008: pd.date_range('2008-12', '2009-04'),\n",
    "    2010: pd.date_range('2010-12', '2011-04')\n",
    "}\n",
    "for i in range(1, 6):\n",
    "    if DATA == 'raw':\n",
    "        if USE_TAKENS:\n",
    "            np.save(\n",
    "                f'../../../data/vandermeer/pickles/raw/10perc/t_mean/takens/eignpy_{i}_takens_.npy',\n",
    "                eigv_time_mean_nlsa[i])\n",
    "            for year in winters:\n",
    "                np.save(\n",
    "                    f'../../../data/vandermeer/pickles/raw/10perc/t_mean/takens/winter_eignpy_{i}_takens_{year}.npy',\n",
    "                    eigv_time_mean_nlsa[i][winters[year]])\n",
    "        else:\n",
    "            np.save(PATH1 + f't_mean/eignpy_{i}.npy', eigv_time_mean[i])\n",
    "            for year in winters:\n",
    "                np.save(PATH1 + f't_mean/winter_eignpy_{i}_{year}.npy',\n",
    "                        eigv_time_mean[i][winters[year]])\n",
    "    else:\n",
    "        if USE_TAKENS:\n",
    "            np.save(\n",
    "                f'../../../data/vandermeer/pickles/anomalies/10perc/t_mean/takens/eignpy_{i}_takens_an.npy',\n",
    "                eigv_time_mean_nlsa[i])\n",
    "            for year in winters:\n",
    "                np.save(\n",
    "                    f'../../../data/vandermeer/pickles/anomalies/10perc/t_mean/takens/winter_eignpy_{i}_takens_an_{year}.npy',\n",
    "                    eigv_time_mean_nlsa[i][winters[year]])\n",
    "        else:\n",
    "            np.save(PATH1 + f't_mean/eignpy_{i}_an.npy', eigv_time_mean[i])\n",
    "            for year in winters:\n",
    "                np.save(PATH1 + f't_mean/winter_eignpy_{i}_an_{year}.npy',\n",
    "                        eigv_time_mean[i][winters[year]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow because of the range of indices I took to create the time-series, the first day of october is also added, which is wrong, so we remove it this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(BEGIN_YEAR, END_YEAR)\n",
    "years = [str(y) for y in years]\n",
    "for y in years[1:]:\n",
    "    i = eigv_time_simple_nlsa[f'{y}-10'].index\n",
    "    j = eigv_time_mean_nlsa[f'{y}-10'].index\n",
    "    eigv_time_simple_nlsa.drop(i, inplace = True)\n",
    "    eigv_time_mean_nlsa.drop(j, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot time-series:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special SSW trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(BEGIN_YEAR, END_YEAR)\n",
    "years = [str(y) for y in years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "dates = [['1980-02-29-12:00:00', 'D'],\n",
    "         ['1981-03-04-12:00:00', 'D'], ['1981-12-04-12:00:00', 'D'],\n",
    "         ['1984-02-24-12:00:00', 'D'], ['1985-01-01-12:00:00', 'S'],\n",
    "         ['1987-01-23-12:00:00', 'D'], ['1987-12-08-12:00:00', 'S'],\n",
    "         ['1988-03-14-12:00:00', 'S'], ['1989-02-21-12:00:00', 'S'],\n",
    "         ['1998-12-15-12:00:00', 'D'], ['1999-02-26-12:00:00', 'S'],\n",
    "         ['2000-03-20-12:00:00', 'D'], ['2001-02-11-12:00:00', 'S'],\n",
    "         ['2001-12-30-12:00:00', 'D'], ['2003-01-18-12:00:00', 'S'],\n",
    "         ['2004-01-05-12:00:00', 'D'], ['2006-01-21-12:00:00', 'D'],\n",
    "         ['2007-02-24-12:00:00', 'D'], ['2008-02-22-12:00:00', 'D'],\n",
    "         ['2009-01-24-12:00:00', 'S'], ['2010-02-09-12:00:00', 'S'],\n",
    "         ['2013-01-07-12:00:00', 'S']]\n",
    "\n",
    "dates_ = []\n",
    "for el in dates:\n",
    "    year = str(pd.to_datetime(el[0]).year)\n",
    "    two_months = [\n",
    "        str(pd.to_datetime(el[0]) + relativedelta(months=2)), el[1] + '_TM'\n",
    "    ]\n",
    "    dates_.append(el)\n",
    "    if (pd.to_datetime(el[0]) + relativedelta(months=2)).month < 4 and (\n",
    "            pd.to_datetime(el[0]) + relativedelta(months=2)).month >= 1:\n",
    "        dates_.append(two_months)\n",
    "    if (pd.to_datetime(el[0]) + relativedelta(months=2)).month >= 10:\n",
    "        dates_.append(two_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_in_d = {}\n",
    "for el in dates_:\n",
    "    year = str(pd.to_datetime(el[0]).year)\n",
    "    if year not in years_in_d:\n",
    "        years_in_d[year] = [[el[0], el[1]]]\n",
    "    else:\n",
    "        l = [j for j in years_in_d[year]]\n",
    "        l.append([el[0], el[1]])\n",
    "        years_in_d[year] = l\n",
    "years_in_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two month trajectories:\n",
    "trajectories = []\n",
    "for el in dates:\n",
    "    year = str(pd.to_datetime(el[0]).year)\n",
    "    two_months = [str(pd.to_datetime(el[0]) + relativedelta(months=2)), el[1] + '_TM']\n",
    "    \n",
    "    if pd.to_datetime(el[0]).month < 4:\n",
    "        if (pd.to_datetime(el[0]) + relativedelta(months=2)).month < 4:\n",
    "            trajectories.append([el, two_months])\n",
    "        else:\n",
    "            end_of_month = [f'{year}-03-31-12:00:00', el[1] + '_TM']\n",
    "            trajectories.append([el, end_of_month])           \n",
    "    else: \n",
    "        trajectories.append([el, two_months])\n",
    "        \n",
    "traj = {}\n",
    "for el in trajectories:\n",
    "    start = el[0]\n",
    "    end = el[1]\n",
    "    year = str(pd.to_datetime(start[0]).year)\n",
    "    range_ = pd.date_range(str(pd.to_datetime(start[0]).year) + '-' +\n",
    "                               str(pd.to_datetime(start[0]).month) + '-' +\n",
    "                               str(pd.to_datetime(start[0]).day),\n",
    "                               str(pd.to_datetime(end[0]).year) + '-' +\n",
    "                               str(pd.to_datetime(end[0]).month) + '-' +\n",
    "                               str(pd.to_datetime(end[0]).day),\n",
    "                               freq='6H')\n",
    "    type_ = el[0][1]\n",
    "    if year not in traj:\n",
    "        traj[year] = [[range_, type_]]\n",
    "    else:\n",
    "        l = [j for j in traj[year]]\n",
    "        l.append([range_, type_])\n",
    "        traj[year] = l\n",
    "traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traj -30 days + 10 days:\n",
    "trajectories_30_10 = []\n",
    "\n",
    "for el in dates:\n",
    "    year = str(pd.to_datetime(el[0]).year)\n",
    "    ten_days = [str(pd.to_datetime(el[0]) + relativedelta(days=10)), el[1] + '_TM']\n",
    "    one_month = [str(pd.to_datetime(el[0]) - relativedelta(days=30)), el[1]]\n",
    "    \n",
    "    # in case we're in jan-april:\n",
    "    if pd.to_datetime(el[0]).month < 4:\n",
    "        if (pd.to_datetime(el[0]) + relativedelta(days=10)).month < 4:\n",
    "            trajectories_30_10.append([one_month, ten_days])\n",
    "        else:\n",
    "            end_of_month = [f'{year}-03-31-12:00:00', el[1] + '_TM']\n",
    "            trajectories_30_10.append([one_month, end_of_month])           \n",
    "    \n",
    "    # in case we're in oct-dec:\n",
    "    else: \n",
    "        if (pd.to_datetime(el[0]) - relativedelta(days=30)).month >=12: \n",
    "            trajectories_30_10.append([one_month, ten_days])\n",
    "        else:\n",
    "            start_of_month = [f'{year}-12-01-12:00:00', el[1]]\n",
    "            trajectories_30_10.append([start_of_month, ten_days])\n",
    "traj_30_10 = {}\n",
    "for el in trajectories_30_10:\n",
    "    start = el[0]\n",
    "    end = el[1]\n",
    "    year = str(pd.to_datetime(start[0]).year)\n",
    "    range_ = pd.date_range(str(pd.to_datetime(start[0]).year) + '-' +\n",
    "                               str(pd.to_datetime(start[0]).month) + '-' +\n",
    "                               str(pd.to_datetime(start[0]).day),\n",
    "                               str(pd.to_datetime(end[0]).year) + '-' +\n",
    "                               str(pd.to_datetime(end[0]).month) + '-' +\n",
    "                               str(pd.to_datetime(end[0]).day),\n",
    "                               freq='6H')\n",
    "    type_ = el[0][1]\n",
    "    if year not in traj_30_10:\n",
    "        traj_30_10[year] = [[range_, type_]]\n",
    "    else:\n",
    "        l = [j for j in traj_30_10[year]]\n",
    "        l.append([range_, type_])\n",
    "        traj_30_10[year] = l\n",
    "traj_30_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_points = {\n",
    "    'D': 'orangered',\n",
    "    'D_TM': 'darkred',\n",
    "    'S': 'deepskyblue',\n",
    "    'S_TM': 'darkblue'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diff(li1, li2):\n",
    "    return (list(list(set(li1) - set(li2)) + list(set(li2) - set(li1))))\n",
    "\n",
    "winters_S, winters_D, winters_ssw = [], [], []\n",
    "for d in dates:\n",
    "    year = pd.to_datetime(d[0]).year\n",
    "    month = pd.to_datetime(d[0]).month\n",
    "    type_ = d[1]\n",
    "    if month < 10 and year not in winters_ssw:\n",
    "        winters_ssw.append(year - 1)\n",
    "        if type_ == 'S':\n",
    "            winters_S.append(year - 1)\n",
    "        else:\n",
    "            winters_D.append(year - 1)\n",
    "    if month >= 10 and year not in winters_ssw:\n",
    "        winters_ssw.append(year)\n",
    "        if type_ == 'S':\n",
    "            winters_S.append(year)\n",
    "        else:\n",
    "            winters_D.append(year)\n",
    "            \n",
    "winters_ssw = np.array(winters_ssw)\n",
    "winters_S = np.array(winters_S)\n",
    "winters_D = np.array(winters_D)\n",
    "\n",
    "winters_no_ssw = np.sort(Diff(range(1979, 2017, 1), winters_ssw))\n",
    "print('Winters with SSW:')\n",
    "print(winters_ssw)\n",
    "print('Winters with SSW type split:')\n",
    "print(winters_S)\n",
    "print('Winters with SSW type displacement:')\n",
    "print(winters_D)\n",
    "print('Winters without SSW:')\n",
    "print(winters_no_ssw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eigenvalues(w_mean, w_simple, w_mean_nlsa, w_simple_nlsa,\n",
    "                     NUM_EIGENVALUES):\n",
    "    fig, axs = plt.subplots(1, figsize=(10, 5))\n",
    "    axs.scatter(x=range(0, NUM_EIGENVALUES),\n",
    "                y=w_mean_nlsa,\n",
    "                marker='o',\n",
    "                alpha=0.7,\n",
    "                label='heat kernel NLSA')\n",
    "        \n",
    "    axs.scatter(x=range(0, NUM_EIGENVALUES),\n",
    "                y=w_mean,\n",
    "                marker='o',\n",
    "                alpha=0.7,\n",
    "                label='heat kernel')\n",
    "    axs.set_ylabel('Eigenvalues')\n",
    "    axs.set_xlabel('Number of eigenvalue')\n",
    "    plt.locator_params(axis='x', nbins=21)\n",
    "    axs.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eigenvalues(w_mean, w_simple, w_mean_nlsa, w_simple_nlsa, NUM_EIGENVALUES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Cluster on january-february only:\n",
    "jan_feb = pd.DataFrame()\n",
    "\n",
    "y = years[1]\n",
    "range_ = pd.date_range(f'{y}-01-01', f'{y}-03-01', freq='6H')\n",
    "jan_feb = eigv_time_mean_nlsa.loc[range_]\n",
    "\n",
    "for y in years[2:]:\n",
    "    range_ = pd.date_range(f'{y}-01-01', f'{y}-03-01', freq='6H')\n",
    "    jan_feb = pd.concat([jan_feb, eigv_time_mean_nlsa.loc[range_]], axis=0)\n",
    "\n",
    "# take first five eigenvectors\n",
    "df_ = jan_feb[[1, 2, 3, 4, 5]]\n",
    "df = np.real(df_.values)\n",
    "spectral = SpectralClustering(n_clusters=NUM_CLUSTERS, random_state=0,\n",
    "                              affinity='rbf').fit(df)\n",
    "\n",
    "jan_feb['cluster_label_spectral_kernel'] = spectral.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df = np.real(eigv_time_mean_nlsa.values)\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(df)\n",
    "\n",
    "eigv_time_mean_nlsa['cluster_label'] = kmeans.labels_\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "df = np.real(eigv_time_mean_nlsa.values)\n",
    "spectral = SpectralClustering(n_clusters=NUM_CLUSTERS, random_state=0,\n",
    "                              affinity='rbf').fit(df)\n",
    "\n",
    "eigv_time_mean_nlsa['cluster_label_spectral_kernel'] = spectral.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from tslearn.clustering import KernelKMeans\n",
    "from tslearn.datasets import CachedDatasets\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "\n",
    "\n",
    "# take first five eigenvectors\n",
    "df_ = jan_feb[[1, 2, 3, 4, 5]]\n",
    "df = np.real(df_.values)\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "gak_km = KernelKMeans(n_clusters=NUM_CLUSTERS,\n",
    "                      kernel=\"gak\",\n",
    "                      kernel_params={\"sigma\": \"auto\"},\n",
    "                      n_init=1,\n",
    "                      verbose=True,\n",
    "                      random_state=seed)\n",
    "y_pred = gak_km.fit_predict(df)\n",
    "eigv_time_mean_nlsa['cluster_label_kernel'] = y_pred.labels_\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all eigenvectors for all years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_eig_clusters(eigv_time, jan_feb, clusters = False, traj = traj):\n",
    "    fig, axs = plt.subplots(10, 1, figsize=(20, 25))\n",
    "    m = 1\n",
    "    for i in range(10):\n",
    "        \n",
    "        full_winters = pd.date_range('1980-01-01', '2017-12-31', freq = '6H')\n",
    "        df = pd.DataFrame(index=full_winters,\n",
    "                          data={\n",
    "                              m:\n",
    "                              eigv_time_mean_nlsa[m],\n",
    "                              'cluster_label_spectral_kernel':\n",
    "                              eigv_time_mean_nlsa['cluster_label_spectral_kernel']\n",
    "                          })        \n",
    "        axs[i].plot(df[m], alpha=0.5, color = 'grey')\n",
    "        colors = {0: 'blue', 1: 'red', 2: 'green'}\n",
    "        \n",
    "        if i%2 == 0:\n",
    "            # All clusters:\n",
    "            draw_clusters = df.dropna()\n",
    "            labels = jan_feb['cluster_label_spectral_kernel']\n",
    "            axs[i].scatter(jan_feb[m].index,\n",
    "                                       jan_feb[m].values,marker=\".\",s = 6,\n",
    "                                       c=labels.apply(lambda x: colors[x]))\n",
    "        \n",
    "            axs[i].set_ylabel(f\"eigenvectorÂ {m}\")\n",
    "            xticks = pd.date_range('1979', '2021', freq='1Y')\n",
    "            xticks_ = [i.year+1 for i in xticks]\n",
    "            axs[i].set_xticks(xticks)\n",
    "            axs[i].set_xticklabels(xticks_)\n",
    "            axs[i].set_xlim([pd.to_datetime('1979'), pd.to_datetime('2019')])\n",
    "        \n",
    "        # Clusters for traj:\n",
    "        else:\n",
    "            for year in traj:\n",
    "                for el in traj[str(year)]:\n",
    "                    if el[0].month[0] < 4:\n",
    "                        if el[0].month[-1] < 3:\n",
    "                            range_ = el[0]\n",
    "                        else: \n",
    "                            y = el[0][0].year\n",
    "                            month = el[0][0].month\n",
    "                            d = el[0][0].day\n",
    "                            range_ = pd.date_range(f'{y}-{month}-{d}', f'{y}-03-01', freq = '6H')\n",
    "                        trajectory = jan_feb[m][range_]\n",
    "                        type_ = el[1]\n",
    "                        labels = jan_feb.loc[trajectory.index]['cluster_label_spectral_kernel']\n",
    "                        axs[i].scatter(trajectory.index,\n",
    "                                           trajectory.values,marker=\".\", s = 6,\n",
    "                                           c=labels.apply(lambda x: colors[x]))\n",
    "\n",
    "            axs[i].set_ylabel(f\"eigenvectorÂ {m}\")\n",
    "            xticks = pd.date_range('1979', '2021', freq='1Y')\n",
    "            xticks_ = [i.year+1 for i in xticks]\n",
    "            axs[i].set_xticks(xticks)\n",
    "            axs[i].set_xticklabels(xticks_)\n",
    "            axs[i].set_xlim([pd.to_datetime('1979'), pd.to_datetime('2019')])\n",
    "            m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_lines ={'S':'darkblue', 'D':'darkred', 'D+S':'darkorange', 'N':'green'}\n",
    "\n",
    "plot_all_eig_clusters(eigv_time_mean_nlsa,jan_feb, traj = traj_30_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_eig(eigv_time, colors_lines, traj = traj):\n",
    "    fig, axs = plt.subplots(10, 1, figsize=(25, 25))\n",
    "    m = 1\n",
    "    for i in range(10):\n",
    "        \n",
    "        \n",
    "        full_winers = pd.date_range('1980-01-01', '2017-12-31', freq = '6H')\n",
    "        df = pd.DataFrame(index=full_winers,\n",
    "                          data={\n",
    "                              m:\n",
    "                              eigv_time_mean_nlsa[m],\n",
    "                              'cluster_label_spectral_kernel':\n",
    "                              eigv_time_mean_nlsa['cluster_label_spectral_kernel']\n",
    "                          })        \n",
    "        axs[i].plot(df[m], alpha=0.5, color = 'grey')\n",
    "        colors = ['blue', 'red','green']\n",
    "        \n",
    "        for year in traj:\n",
    "            for el in traj[str(year)]:\n",
    "                trajectory = eigv_time[m][el[0]]\n",
    "                type_ = el[1]\n",
    "                labels = eigv_time.loc[trajectory.index]['cluster_label_spectral_kernel']\n",
    "\n",
    "                axs[i].plot(trajectory.index,trajectory.values,\n",
    "                                    alpha=1,color = colors_lines[type_],\n",
    "                                    linewidth=0.8)\n",
    "        axs[i].set_ylabel(f\"eigenvectorÂ {m}\")\n",
    "        xticks = pd.date_range('1979', '2021', freq='1Y')\n",
    "        xticks_ = [i.year+1 for i in xticks]\n",
    "        axs[i].set_xticks(xticks)\n",
    "        axs[i].set_xticklabels(xticks_)\n",
    "        axs[i].set_xlim([pd.to_datetime('1979'), pd.to_datetime('2019')])\n",
    "        m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_all_eig(eigv_time_mean_nlsa, colors_lines, traj = traj_30_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average trajectories with and without SSW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diff(li1, li2):\n",
    "    return (list(list(set(li1) - set(li2)) + list(set(li2) - set(li1))))\n",
    "\n",
    "winters_S, winters_D, winters_ssw = [], [], []\n",
    "for d in dates:\n",
    "    year = pd.to_datetime(d[0]).year\n",
    "    month = pd.to_datetime(d[0]).month\n",
    "    type_ = d[1]\n",
    "    if month < 10 and year not in winters_ssw:\n",
    "        winters_ssw.append(year - 1)\n",
    "        if type_ == 'S':\n",
    "            winters_S.append(year - 1)\n",
    "        else:\n",
    "            winters_D.append(year - 1)\n",
    "    if month >= 10 and year not in winters_ssw:\n",
    "        winters_ssw.append(year)\n",
    "        if type_ == 'S':\n",
    "            winters_S.append(year)\n",
    "        else:\n",
    "            winters_D.append(year)\n",
    "            \n",
    "winters_ssw = np.unique(np.array(winters_ssw))\n",
    "winters_S = np.unique(np.array(winters_S))\n",
    "winters_D = np.unique(np.array(winters_D))\n",
    "\n",
    "winters_no_ssw = np.sort(Diff(range(1979, 2017, 1), winters_ssw))\n",
    "print('Winters with SSW:')\n",
    "print(winters_ssw)\n",
    "print('Winters with SSW type split:')\n",
    "print(winters_S)\n",
    "print('Winters with SSW type displacement:')\n",
    "print(winters_D)\n",
    "print('Winters without SSW:')\n",
    "print(winters_no_ssw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_traj(eig, winters_years):\n",
    "    # first winter:\n",
    "    year = winters_years[0]\n",
    "    winter_of_that_year = pd.date_range(str(year) + '-12',\n",
    "                                        str(year + 1) + '-04',\n",
    "                                        freq='6H')\n",
    "    winter_eig = eig[winter_of_that_year][:485]\n",
    "\n",
    "    df = pd.DataFrame(index=transform_date(winter_eig.index)[:485],\n",
    "                      data={year: winter_eig.values})\n",
    "    for year in winters_years[1:]:\n",
    "        winter_of_that_year = pd.date_range(str(year) + '-12',\n",
    "                                            str(year + 1) + '-04',\n",
    "                                            freq='6H')\n",
    "        val = eig[winter_of_that_year][:485].values\n",
    "\n",
    "        df[year] = eig[winter_of_that_year][:485].values\n",
    "\n",
    "    return np.mean(df, axis=1), df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traj_SSW_spring(type_ = 'S', winters_S = winters_S):\n",
    "    trajectories = []\n",
    "    for winter in winters_S:\n",
    "        if str(winter) in traj_30_10:\n",
    "            for traj in traj_30_10[str(winter)]:\n",
    "                month_1 = traj[0].month[0]\n",
    "                # if trajectory of ssw starts before march \n",
    "                if month_1 <= 3 and traj[1] in type_:\n",
    "                    trajectories.append(traj)\n",
    "    return trajectories\n",
    "\n",
    "traj_S_spring = traj_SSW_spring(type_ = 'S', winters_S = winters_S)\n",
    "traj_D_spring = traj_SSW_spring(type_ = 'D', winters_S = winters_D)\n",
    "traj_ssw_spring = traj_SSW_spring(type_ = ['D','S'], winters_S = winters_ssw)\n",
    "\n",
    "traj_no_ssw_spring = []\n",
    "for winter in winters_no_ssw:\n",
    "    traj_no_ssw_spring.append([\n",
    "        pd.date_range(str(winter) + '-01', str(winter) + '-04', freq='6H'), 'N'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traj_SSW_winter(type_ = 'S', winters_S = winters_S):\n",
    "    trajectories = []\n",
    "    for winter in winters_S:\n",
    "        if str(winter) in traj_30_10:\n",
    "            for traj in traj_30_10[str(winter)]:\n",
    "                month_1 = traj[0].month[0]\n",
    "                # if trajectory of ssw starts before march \n",
    "                if month_1 > 4 and traj[1] in type_:\n",
    "                    trajectories.append(traj)\n",
    "    return trajectories\n",
    "\n",
    "traj_S_winter = traj_SSW_winter(type_ = 'S', winters_S = winters_S)\n",
    "traj_D_winter = traj_SSW_winter(type_ = 'D', winters_S = winters_D)\n",
    "traj_ssw_winter = traj_SSW_winter(type_ = ['D','S'], winters_S = winters_ssw)\n",
    "\n",
    "traj_no_ssw_winter = []\n",
    "for winter in winters_no_ssw:\n",
    "    traj_no_ssw_winter.append([\n",
    "        pd.date_range(str(winter) + '-12-01', str(winter) + '-12-31', freq='6H'), 'N'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_traj_spring(eig, trajectories):\n",
    "    m = 1\n",
    "    winter_eig = eig[trajectories[0][0]]\n",
    "    ind = transform_date(winter_eig.index, special_traj = True)\n",
    "    test = pd.Series(index = ind, data = winter_eig.values)\n",
    "    if pd.datetime(1984,2,28) in test.index:\n",
    "        test.drop(test['1984-02-28'].index, axis = 0, inplace = True)\n",
    "    df = pd.DataFrame(index = pd.date_range(str(1984) + '-01', str(1984) + '-04',\n",
    "                                                freq='6H'), data = {1: test})\n",
    "    n = 2\n",
    "    for traj in trajectories[1:]: \n",
    "        winter_eig = eig[traj[0]]\n",
    "\n",
    "        ind = transform_date(winter_eig.index, special_traj = True)\n",
    "        test = pd.Series(index = ind, data = winter_eig.values)\n",
    "        if pd.datetime(1984,2,28) in test.index:\n",
    "            test.drop(test['1984-02-28'].index, axis = 0, inplace = True)\n",
    "        df[n] = test\n",
    "        n+= 1\n",
    "    \n",
    "    return np.mean(df, axis=1), df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_traj_winter(eig, trajectories):\n",
    "    m = 1\n",
    "    winter_eig = eig[trajectories[0][0]]\n",
    "    ind = transform_date(winter_eig.index, special_traj = True)\n",
    "    test = pd.Series(index = ind, data = winter_eig.values)\n",
    "\n",
    "    df = pd.DataFrame(index = pd.date_range(str(1983) + '-12-01', str(1983) + '-12-31',\n",
    "                                                freq='6H'), data = {1: test})\n",
    "    n = 2\n",
    "    for traj in trajectories[1:]: \n",
    "        winter_eig = eig[traj[0]]\n",
    "\n",
    "        ind = transform_date(winter_eig.index, special_traj = True)\n",
    "        test = pd.Series(index = ind, data = winter_eig.values)\n",
    "        df[n] = test\n",
    "        n+= 1\n",
    "    \n",
    "    return np.mean(df, axis=1), df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date(column, special_traj=False):\n",
    "    \n",
    "    if special_traj:\n",
    "        new_col = []\n",
    "        month = column[0].month\n",
    "        for i in column:\n",
    "            month = i.month\n",
    "            day = i.day\n",
    "            if month < 4:\n",
    "                if month == 2 and day == 29:\n",
    "                    new_col.append(\n",
    "                        datetime(1984, month, day - 1, i.hour, i.minute,\n",
    "                                 i.second))\n",
    "                else:\n",
    "                    new_col.append(\n",
    "                        datetime(1984, month, day, i.hour, i.minute, i.second))\n",
    "            else:\n",
    "                new_col.append(\n",
    "                    datetime(1983, month, day, i.hour, i.minute, i.second))\n",
    "    else:\n",
    "        new_col = []\n",
    "        year = column[0].year\n",
    "        for i in column:\n",
    "            month = i.month\n",
    "            day = i.day\n",
    "            if i.year > year:\n",
    "                if month == 2 and day == 29:\n",
    "                    new_col.append(\n",
    "                        datetime(1984, month, day - 1, i.hour, i.minute,\n",
    "                                 i.second))\n",
    "                else:\n",
    "                    new_col.append(\n",
    "                        datetime(1984, month, day, i.hour, i.minute, i.second))\n",
    "            else:\n",
    "                new_col.append(\n",
    "                    datetime(1983, month, day, i.hour, i.minute, i.second))\n",
    "    return new_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms according to clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_years(df, winters):\n",
    "    winters_ = [str(i) for i in winters]\n",
    "    df_ = df[winters_[0]]\n",
    "    for y in winters_[1:]:\n",
    "        df_ = pd.concat([df_, df[y]], axis=1)\n",
    "    return df_\n",
    "\n",
    "\n",
    "df_ssw = select_years(eigv_time_mean_nlsa, winters_ssw)\n",
    "df_no_ssw = select_years(eigv_time_mean_nlsa, winters_no_ssw)\n",
    "df_ssw_D = select_years(eigv_time_mean_nlsa, winters_D)\n",
    "df_ssw_S = select_years(eigv_time_mean_nlsa, winters_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(10, 3, figsize=(30, 30))\n",
    "\n",
    "m = 1\n",
    "n = 0\n",
    "for j in range(10):\n",
    "    for n in range(3):\n",
    "        avg_no_ssw, df = average_traj(eigv_time_mean_nlsa[m], winters_no_ssw)\n",
    "        avg_ssw, df = average_traj(eigv_time_mean_nlsa[m], winters_ssw)\n",
    "        avg_ssw_D, df = average_traj(eigv_time_mean_nlsa[m], winters_D)\n",
    "        avg_ssw_S, df = average_traj(eigv_time_mean_nlsa[m], winters_S)\n",
    "        if n == 0:\n",
    "            sns.distplot(df_no_ssw[m].values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='No-SSW',\n",
    "                         kde=True,\n",
    "                         hist=False,color='black')\n",
    "            sns.distplot(df_ssw[m].values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='SSW',\n",
    "                         kde=True,\n",
    "                         hist=False)\n",
    "            sns.distplot(df_ssw_D[m].values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='SSW-D',\n",
    "                         kde=True,\n",
    "                         hist=False)\n",
    "            sns.distplot(df_ssw_S[m].values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='SSW-S',\n",
    "                         kde=True,\n",
    "                         hist=False)\n",
    "            ax[j, n].set_title(f'Histogram for all winters, Eig {m}')\n",
    "            ax[j, n].ticklabel_format(style='sci', axis='x', scilimits=(0, 0))\n",
    "            ax[j, n].legend(loc='upper left')\n",
    "        \n",
    "        # Plot clusters for january-february period:\n",
    "        if n == 2:\n",
    "            c1 = jan_feb[jan_feb['cluster_label_spectral_kernel'] == 0]\n",
    "            c2 = jan_feb[jan_feb['cluster_label_spectral_kernel'] == 1]\n",
    "            c3 = jan_feb[jan_feb['cluster_label_spectral_kernel'] == 2]\n",
    "            sns.distplot(jan_feb[m].values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='all',\n",
    "                         color='black',\n",
    "                         kde=True,\n",
    "                         hist=False)\n",
    "            sns.distplot(c1[m].values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='c1',\n",
    "                         kde=True,\n",
    "                         hist=False)\n",
    "            sns.distplot(c2[m].values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='c2',\n",
    "                         kde=True,\n",
    "                         hist=False)\n",
    "            sns.distplot(c3[m].values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='c3',\n",
    "                         kde=True,\n",
    "                         hist=False)\n",
    "            ax[j, n].set_title(f'Histogram for clusters on jan-feb Eig {m}')\n",
    "            ax[j, n].legend(loc='upper left')\n",
    "            m += 1\n",
    "        \n",
    "        if n == 1:\n",
    "            sns.distplot(avg_no_ssw.values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='No-SSW',\n",
    "                         kde=True,\n",
    "                         hist=False,color='black')\n",
    "            sns.distplot(avg_ssw.values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='SSW',\n",
    "                         kde=True,\n",
    "                         hist=False)\n",
    "            sns.distplot(avg_ssw_D.values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='SSW-D',\n",
    "                         kde=True,\n",
    "                         hist=False)\n",
    "            sns.distplot(avg_ssw_S.values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='SSW-S',\n",
    "                         kde=True,\n",
    "                         hist=False)\n",
    "            ax[j, n].set_title(f'Eigenvector {m}')\n",
    "            ax[j, n].ticklabel_format(style='sci', axis='x', scilimits=(0, 0))\n",
    "            ax[j, n].set_title(f'Histogram for average winter Eig {m}')\n",
    "            ax[j, n].legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension reduction, PCA on one winter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_winters(eigv, years):\n",
    "    pd.DataFrame()\n",
    "    # get winter dates:\n",
    "    winter = pd.date_range(years[m] + '-12',\n",
    "                                   years[m + 1] + '-04',\n",
    "                                   freq='6H')\n",
    "    #select time series for that winter:\n",
    "    winter_eig_mean = eigv[winter]\n",
    "    winter_eig_simple = eigv[winter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_winters(eigv_time_mean_nlsa[1], years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One winter NLSA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_winter_nlsa(eigv_time, eigv_time_simple, sign_mean, sign_simple,\n",
    "                         year):\n",
    "    fig, axs = plt.subplots(4, 5, figsize=(23, 20))\n",
    "    m = 1\n",
    "    for i in range(4):\n",
    "        for j in range(5):\n",
    "            winter_2008 = pd.date_range(str(year) + '-12',\n",
    "                                        str(year + 1) + '-04',\n",
    "                                        freq='6H')\n",
    "            winter_2008_eig_mean = eigv_time[m][winter_2008]\n",
    "            winter_2008_eig_simple = eigv_time_simple[m][winter_2008]\n",
    "\n",
    "            axs[i, j].plot(winter_2008_eig_mean.index,\n",
    "                           sign_mean[m] * winter_2008_eig_mean.values,\n",
    "                           label='heat kernel')\n",
    "            axs[i, j].plot(winter_2008_eig_simple.index,\n",
    "                           sign_simple[m] * winter_2008_eig_simple.values,\n",
    "                           label='simple')\n",
    "            axs[i, j].set_title(f\"EigenvectorÂ {m}\")\n",
    "            axs[i, j].set_xticks((str(year) + '-12', str(year + 1) + '-01',\n",
    "                                  str(year + 1) + '-02', str(year + 1) + '-03',\n",
    "                                  str(year + 1) + '-04'))\n",
    "            axs[i, j].set_xticklabels(['Dec', 'Jan', 'Feb', 'Mar', 'Apr'])\n",
    "            m += 1\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLSA versus Laplacian Eigenmaps: \n",
    "sign_mean = [0, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1]\n",
    "sign_mean_nlsa = [0, -1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1]\n",
    "plot_one_winter_nlsa(eigv_time_mean_nlsa, eigv_time_mean,\n",
    "                     sign_mean_nlsa, sign_mean, 2008)\n",
    "plt.legend(['NLSA', 'Eigenmaps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual winters: \n",
    "\n",
    "Plot each winter for an eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_each_winter_nlsa(eigv_time,\n",
    "                          eigv_time_simple,\n",
    "                          sign_mean,\n",
    "                          sign_simple,\n",
    "                          years,\n",
    "                          num_eig=1, colors_points = colors_points):\n",
    "    fig, axs = plt.subplots(10, 4, figsize=(30, 35))\n",
    "    m = 0\n",
    "    min_ = np.min(eigv_time[num_eig])\n",
    "    max_ = np.max(eigv_time[num_eig])\n",
    "    for i in range(10):\n",
    "        for j in range(4):\n",
    "            if m == 39:\n",
    "                break\n",
    "            # get winter dates:\n",
    "            winter = pd.date_range(years[m] + '-12',\n",
    "                                   years[m + 1] + '-04',\n",
    "                                   freq='6H')\n",
    "            # select time series for that winter:\n",
    "            winter_eig_mean = eigv_time[num_eig][winter]\n",
    "            winter_eig_simple = eigv_time_simple[num_eig][winter]\n",
    "\n",
    "            axs[i, j].plot(winter_eig_mean.index,\n",
    "                           sign_mean * winter_eig_mean.values)\n",
    "            axs[i, j].plot(winter_eig_simple.index,\n",
    "                           sign_simple * winter_eig_simple.values)\n",
    "\n",
    "            # add end of year dates (Dec):\n",
    "            if years[m] in years_in_d:\n",
    "                for el in years_in_d[years[m]]:\n",
    "                    d_ = el[0]\n",
    "                    if pd.to_datetime(d_).month > 10:\n",
    "                        axs[i, j].plot(pd.to_datetime(d_),\n",
    "                                       sign_mean * winter_eig_mean[d_],\n",
    "                                       'X',\n",
    "                                       color=colors_points[el[1]])\n",
    "                        axs[i, j].annotate(d_[:-9],\n",
    "                                           (pd.to_datetime(d_),\n",
    "                                            sign_mean * winter_eig_mean[d_]))\n",
    "            # add beginning of next year dates (Jan-Apr):\n",
    "            if years[m + 1] in years_in_d:\n",
    "                for el in years_in_d[years[m + 1]]:\n",
    "                    d_ = el[0]\n",
    "                    if pd.to_datetime(d_).month < 4:\n",
    "                        axs[i, j].plot(pd.to_datetime(d_),\n",
    "                                       sign_mean * winter_eig_mean[d_],\n",
    "                                       'X',\n",
    "                                       color=colors_points[el[1]])\n",
    "                        axs[i, j].annotate(d_[:-9],\n",
    "                                           (pd.to_datetime(d_),\n",
    "                                            sign_mean * winter_eig_mean[d_]))\n",
    "\n",
    "            axs[i, j].set_title(\"Winter \" + years[m] + '-' + years[m + 1])\n",
    "            axs[i, j].set_xticks(\n",
    "                (years[m] + '-12', years[m + 1] + '-01', years[m + 1] + '-02',\n",
    "                 years[m + 1] + '-03', years[m + 1] + '-04'))\n",
    "            axs[i, j].set_xticklabels(['Dec', 'Jan', 'Feb', 'Mar', 'Apr'])\n",
    "\n",
    "            if sign_mean > 0:\n",
    "                axs[i, j].set_ylim([min_, max_])\n",
    "            else:\n",
    "                axs[i, j].set_ylim([-max_, -min_])\n",
    "            m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_each_winter_nlsa(eigv_time_mean_nlsa, eigv_time_mean, 1,\n",
    "                      1, years, num_eig = 2)\n",
    "plt.legend(['NLSA', 'Eigenmaps'])\n",
    "#plt.suptitle('NLSA winters for first eigenvector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssw = select_years(eigv_time_mean_nlsa, winters_ssw)\n",
    "df_no_ssw = select_years(eigv_time_mean_nlsa, winters_no_ssw)\n",
    "df_ssw_D = select_years(eigv_time_mean_nlsa, winters_D)\n",
    "df_ssw_S = select_years(eigv_time_mean_nlsa, winters_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(10, 3, figsize=(12, 20))\n",
    "\n",
    "m = 1\n",
    "n = 0\n",
    "for j in range(10):\n",
    "    for n in range(3):\n",
    "        eig = eigv_time_mean_nlsa[m]\n",
    "        avg_ssw, df = avg_traj_winter(eig, traj_ssw_winter)\n",
    "        avg_ssw_D, df = avg_traj_winter(eig, traj_D_winter)\n",
    "        avg_ssw_S, df = avg_traj_winter(eig, traj_S_winter)\n",
    "        avg_no_ssw, df = avg_traj_winter(eig, traj_no_ssw_winter)\n",
    "\n",
    "        avg_ssw_spring, df = avg_traj_spring(eig, traj_ssw_spring)\n",
    "        avg_ssw_D_spring, df = avg_traj_spring(eig, traj_D_spring)\n",
    "        avg_ssw_S_spring, df = avg_traj_spring(eig, traj_S_spring)\n",
    "        avg_no_ssw_spring, df = avg_traj_spring(eig, traj_no_ssw_spring)\n",
    "\n",
    "        if n == 0:\n",
    "            sns.distplot(avg_no_ssw.values,\n",
    "                         ax=ax[j, n],\n",
    "                         color='black',\n",
    "                         label='No-SSW',\n",
    "                         kde=True)\n",
    "            sns.distplot(avg_ssw.values, ax=ax[j, n], label='SSW', kde=True)\n",
    "            sns.distplot(avg_ssw_D.values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='SSW-D',\n",
    "                         kde=True)\n",
    "            sns.distplot(avg_ssw_S.values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='SSW-S',\n",
    "                         kde=True)\n",
    "            if j == 0:\n",
    "                ax[j, n].set_title(f'Winter trajectories')\n",
    "            ax[j, n].set_ylabel(f'Eigenvector {m}')\n",
    "            ax[j, n].ticklabel_format(style='sci', axis='x', scilimits=(0, 0))\n",
    "        if n == 1:\n",
    "            sns.distplot(avg_no_ssw_spring.values,\n",
    "                         ax=ax[j, n],\n",
    "                         color='black',\n",
    "                         label='No-SSW',\n",
    "                         kde=True)\n",
    "            sns.distplot(avg_ssw_spring.values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='SSW',\n",
    "                         kde=True)\n",
    "            sns.distplot(avg_ssw_D_spring.values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='SSW-D',\n",
    "                         kde=True)\n",
    "            sns.distplot(avg_ssw_S_spring.values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='SSW-S',\n",
    "                         kde=True)\n",
    "            if j == 0:\n",
    "                ax[j, n].set_title(f'Spring trajectories')\n",
    "            ax[j, n].set_ylabel('')\n",
    "            ax[j, n].ticklabel_format(style='sci', axis='x', scilimits=(0, 0))\n",
    "        if n == 2:\n",
    "            \"\"\"\n",
    "            ax[j, n].plot(avg_no_ssw.dropna().rolling(window=16).mean(),\n",
    "                          color='black')\n",
    "            ax[j, n].plot(avg_ssw.dropna().rolling(window=16).mean(),\n",
    "                          color='C0')\n",
    "            ax[j, n].plot(avg_ssw_D.dropna().rolling(window=16).mean(),\n",
    "                          color='C1')\n",
    "            ax[j, n].plot(avg_ssw_S.dropna().rolling(window=16).mean(),\n",
    "                          color='C2')\n",
    "\n",
    "            ax[j, n].plot(avg_no_ssw_spring.dropna().rolling(window=16).mean(),\n",
    "                          color='black',\n",
    "                          label='No-SSW')\n",
    "            ax[j, n].plot(avg_ssw_spring.dropna().rolling(window=16).mean(),\n",
    "                          color='C0',\n",
    "                          label='SSW')\n",
    "            ax[j, n].plot(avg_ssw_D_spring.dropna().rolling(window=16).mean(),\n",
    "                          color='C1',\n",
    "                          label='SSW-D')\n",
    "            ax[j, n].plot(avg_ssw_S_spring.dropna().rolling(window=16).mean(),\n",
    "                          color='C2',\n",
    "                          label='SSW-S')\"\"\"\n",
    "            \n",
    "            sns.distplot(df_no_ssw[m].values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='No-SSW',\n",
    "                         kde=True,hist = False,color='black')\n",
    "            sns.distplot(df_ssw[m].values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='SSW',\n",
    "                         kde=True, hist = False)\n",
    "            sns.distplot(df_ssw_D[m].values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='SSW-D',\n",
    "                         kde=True, hist = False)\n",
    "            sns.distplot(df_ssw_S[m].values,\n",
    "                         ax=ax[j, n],\n",
    "                         label='SSW-S',\n",
    "                         kde=True, hist = False)\n",
    "            if j == 0:\n",
    "                ax[j, n].set_title(f'Whole winters')\n",
    "            ax[j, n].ticklabel_format(style='sci', axis='x', scilimits=(0, 0))\n",
    "            ax[j, n].legend(loc='upper left')\n",
    "            ax[j, n].set_ylabel('')\n",
    "            \n",
    "            \"\"\"\n",
    "            if j == 0:\n",
    "                ax[j, n].set_title(f'Average trajectory')\n",
    "            \n",
    "            ax[j, n].set_xticks(\n",
    "                ('1983-12', '1984-01', '1984-02', '1984-03', '1984-04'))\n",
    "            ax[j, n].set_xticklabels(['Dec', 'Jan', 'Feb', 'Mar', 'Apr'])\"\"\"\n",
    "            m += 1\n",
    "            ax[j, n].legend(bbox_to_anchor=(1.05, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot overlapping winters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date(column, special_traj=False):\n",
    "    \n",
    "    if special_traj:\n",
    "        new_col = []\n",
    "        month = column[0].month\n",
    "        for i in column:\n",
    "            month = i.month\n",
    "            day = i.day\n",
    "            if month < 4:\n",
    "                if month == 2 and day == 29:\n",
    "                    new_col.append(\n",
    "                        datetime(1984, month, day - 1, i.hour, i.minute,\n",
    "                                 i.second))\n",
    "                else:\n",
    "                    new_col.append(\n",
    "                        datetime(1984, month, day, i.hour, i.minute, i.second))\n",
    "            else:\n",
    "                new_col.append(\n",
    "                    datetime(1983, month, day, i.hour, i.minute, i.second))\n",
    "    else:\n",
    "        new_col = []\n",
    "        year = column[0].year\n",
    "        for i in column:\n",
    "            month = i.month\n",
    "            day = i.day\n",
    "            if i.year > year:\n",
    "                if month == 2 and day == 29:\n",
    "                    new_col.append(\n",
    "                        datetime(1984, month, day - 1, i.hour, i.minute,\n",
    "                                 i.second))\n",
    "                else:\n",
    "                    new_col.append(\n",
    "                        datetime(1984, month, day, i.hour, i.minute, i.second))\n",
    "            else:\n",
    "                new_col.append(\n",
    "                    datetime(1983, month, day, i.hour, i.minute, i.second))\n",
    "    return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_d(date, year):\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "    if month == 2 and day == 29:\n",
    "        return datetime(year, month, day - 1, date.hour, date.minute,\n",
    "                        date.second)\n",
    "    else:\n",
    "        \n",
    "        return datetime(year, month, day, date.hour, date.minute, date.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winters_supp(eigv_time,\n",
    "                 ax,\n",
    "                 sign,\n",
    "                 eig,\n",
    "                 traj,\n",
    "                 colors_lines,\n",
    "                 colors,winters_no_ssw, winters_ssw,\n",
    "                 alpha=0.5,\n",
    "                 linewidth=1,\n",
    "                 add_traj=False,\n",
    "                 y_={\n",
    "                     2010: 'D',\n",
    "                     1998: 'D+S',\n",
    "                     2008: 'S',\n",
    "                     1990: 'N'\n",
    "                 }):\n",
    "    years = eigv_time[eig].index.year.unique()\n",
    "\n",
    "    n = len(years)\n",
    "    i = 1\n",
    "    for year in years[:-1]:\n",
    "        winter_of_that_year = pd.date_range(str(year) + '-12',\n",
    "                                            str(year + 1) + '-04',\n",
    "                                            freq='6H')\n",
    "        winter_eig = eigv_time[eig][winter_of_that_year]\n",
    "        ax.plot(transform_date(winter_eig.index),\n",
    "                sign * winter_eig.values,\n",
    "                color='grey',\n",
    "                alpha=alpha,\n",
    "                linewidth=linewidth)\n",
    "\n",
    "        \"\"\"\n",
    "        # Add dates:\n",
    "        if str(year) in years_in_d:\n",
    "            for el in years_in_d[str(year)]:\n",
    "                d_ = pd.to_datetime(el[0])\n",
    "                if d_.month >= 12:\n",
    "                    ax.plot(transform_d(d_, 1983),\n",
    "                            sign * winter_eig[d_],\n",
    "                            'x',\n",
    "                            color=colors[el[1]])\n",
    "        if str(year + 1) in years_in_d:\n",
    "            for el in years_in_d[str(year + 1)]:\n",
    "                d_ = pd.to_datetime(el[0])\n",
    "                if d_.month < 4:\n",
    "                    ax.plot(transform_d(d_, 1984),\n",
    "                            sign * winter_eig[d_],\n",
    "                            'x',\n",
    "                            color=colors[el[1]])\"\"\"\n",
    "\n",
    "        if year == 1983:\n",
    "            ax.set_xticks(\n",
    "                ('1983-12', '1984-01', '1984-02', '1984-03', '1984-04'))\n",
    "            ax.set_xticklabels(['Dec', 'Jan', 'Feb', 'Mar', 'Apr'])\n",
    "\n",
    "        if year in y_:\n",
    "            ax.plot(transform_date(winter_eig.index),\n",
    "                    sign * winter_eig.values,\n",
    "                    color=colors_lines[y_[year]],\n",
    "                    alpha=0.8,\n",
    "                    linewidth=linewidth,\n",
    "                    label=f'{year}-{year+1}: {y_[year]}')\n",
    "        if add_traj:\n",
    "            if str(year) in traj:\n",
    "                for el in traj[str(year)]:\n",
    "                    trajectory = eigv_time[eig][el[0]]\n",
    "                    type_ = el[1]\n",
    "                    ax.plot(transform_date(trajectory.index,\n",
    "                                           special_traj=True),\n",
    "                            sign * trajectory.values,\n",
    "                            color='purple',\n",
    "                            alpha=0.6,\n",
    "                            linewidth=0.8)\n",
    "        \n",
    "        i += 1\n",
    "    avg_no_ssw, df = average_traj(eigv_time[eig], winters_no_ssw)\n",
    "\n",
    "    eig = eigv_time[eig]\n",
    "    avg_ssw_winter, df = avg_traj_winter(eig, traj_ssw_winter)\n",
    "    avg_ssw_D_winter, df = avg_traj_winter(eig, traj_D_winter)\n",
    "    avg_ssw_S_winter, df = avg_traj_winter(eig, traj_S_winter)\n",
    "    avg_no_ssw_winter, df = avg_traj_winter(eig, traj_no_ssw_winter)\n",
    "    \n",
    "    avg_ssw_spring, df = avg_traj_spring(eig, traj_ssw_spring)\n",
    "    avg_ssw_D_spring, df = avg_traj_spring(eig, traj_D_spring)\n",
    "    avg_ssw_S_spring, df = avg_traj_spring(eig, traj_S_spring)\n",
    "    avg_no_ssw_spring, df = avg_traj_spring(eig, traj_no_ssw_spring)\n",
    "    \"\"\"\n",
    "    ax.plot(avg_no_ssw_winter.index,\n",
    "                sign * avg_no_ssw_winter.values,\n",
    "                linewidth=1, color = 'black', label = 'NO-SSW')\n",
    "    ax.plot(avg_no_ssw_spring.index,\n",
    "                sign * avg_no_ssw_spring.values,\n",
    "                linewidth=1, color = 'black')\n",
    "    ax.plot(avg_ssw_winter.index,\n",
    "                sign * avg_ssw_winter.values,\n",
    "                linewidth=1, color = colors_points['SSW'], label = 'SSW')\n",
    "    ax.plot(avg_ssw_spring.index,\n",
    "                sign * avg_ssw_spring.values,\n",
    "                linewidth=1, color = colors_points['SSW'])\n",
    "    ax.plot(avg_ssw_D_winter.index,\n",
    "                sign * avg_ssw_D_winter.values,\n",
    "                linewidth=1, color = colors_points['D'], label = 'SSW-D')\n",
    "    ax.plot(avg_ssw_D_spring.index,\n",
    "                sign * avg_ssw_D_spring.values,\n",
    "                linewidth=1, color = colors_points['D'])\n",
    "    ax.plot(avg_ssw_S_winter.index,\n",
    "                sign * avg_ssw_S_winter.values,\n",
    "                linewidth=1, color = colors_points['S'], label = 'SSW-S')\n",
    "    ax.plot(avg_ssw_S_spring.index,\n",
    "                sign * avg_ssw_S_spring.values,\n",
    "                linewidth=1, color = colors_points['S'])\"\"\"\n",
    "\n",
    "    plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_lines ={'S':'darkblue', 'D':'darkred', 'D+S':'darkorange', 'N':'green'}\n",
    "colors_points = {\n",
    "    'D': 'purple',\n",
    "    'D_TM': 'darkred',\n",
    "    'S': 'deepskyblue',\n",
    "    'S_TM': 'darkblue', \n",
    "    'c1-SSW':'orange', \n",
    "    'c2-SSW':'orange',\n",
    "    'c1-NO-SSW':'limegreen', \n",
    "    'c2-NO-SSW':'limegreen',\n",
    "    'SSW':'red', \n",
    "    'NO-SSW': 'darkgreen'\n",
    "    \n",
    "}\n",
    "fig, ax = plt.subplots(1, figsize=(10, 5))\n",
    "winters_supp(eigv_time_mean_nlsa,\n",
    "             ax,\n",
    "             1,\n",
    "             eig=1,\n",
    "             traj=traj,\n",
    "             colors_lines=colors_lines,\n",
    "             colors=colors_points,\n",
    "             winters_no_ssw=winters_no_ssw,\n",
    "             winters_ssw=winters_ssw,\n",
    "             add_traj=True, y_ = {})\n",
    "#plt.legend(loc='lower right')\n",
    "plt.title('Overlapping winters for the first Laplacian eigenmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplot: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.collections as mcoll\n",
    "import matplotlib.path as mpath\n",
    "\n",
    "def colorline(x,\n",
    "              y,\n",
    "              ax,\n",
    "              z=None,\n",
    "              cmap=plt.get_cmap('copper'),\n",
    "              norm=plt.Normalize(0.0, 1.0),\n",
    "              linewidth=3,\n",
    "              alpha=1.0):\n",
    "    # Default colors equally spaced on [0,1]:\n",
    "    if z is None:\n",
    "        z = np.linspace(0.0, 1.0, len(x))\n",
    "    # Special case if a single number:\n",
    "    if not hasattr(\n",
    "            z, \"__iter__\"):  # to check for numerical input -- this is a hack\n",
    "        z = np.array([z])\n",
    "    z = np.asarray(z)\n",
    "    segments = make_segments(x, y)\n",
    "    lc = mcoll.LineCollection(segments,\n",
    "                              array=z,\n",
    "                              cmap=cmap,\n",
    "                              norm=norm,\n",
    "                              linewidth=linewidth,\n",
    "                              alpha=alpha)\n",
    "    #ax = plt.gca()\n",
    "    ax.add_collection(lc)\n",
    "    return lc\n",
    "\n",
    "def make_segments(x, y):\n",
    "    \"\"\"\n",
    "    Create list of line segments from x and y coordinates, in the correct format\n",
    "    for LineCollection: an array of the form numlines x (points per line) x 2 (x\n",
    "    and y) array\n",
    "    \"\"\"\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_arrow(line, position=None, direction='right', size=15, color=None, num_arrows = 5,linewidth=1, alpha=1):\n",
    "    \"\"\"\n",
    "    add an arrow to a line.\n",
    "\n",
    "    line:       Line2D object\n",
    "    position:   x-position of the arrow. If None, mean of xdata is taken\n",
    "    direction:  'left' or 'right'\n",
    "    size:       size of the arrow in fontsize points\n",
    "    color:      if None, line color is taken.\n",
    "    \"\"\"\n",
    "    if color is None:\n",
    "        color = line.get_color()\n",
    "\n",
    "    xdata = line.get_xdata()\n",
    "    ydata = line.get_ydata()\n",
    "\n",
    "    min_ = xdata.min()\n",
    "    max_ = xdata.max()\n",
    "    if position is None:\n",
    "        positions = np.linspace(min_, max_, num_arrows)\n",
    "    # find closest index\n",
    "    start_inds = [np.argmin(np.absolute(xdata - p)) for p in positions]\n",
    "    if direction == 'right':\n",
    "        end_inds = [start_ind + 1 for start_ind in start_inds]\n",
    "    else:\n",
    "        end_inds = [start_ind - 1 for start_ind in start_inds]\n",
    "    for i in range(len(start_inds)):\n",
    "        if start_inds[i] < len(xdata) and end_inds[i] < len(xdata): \n",
    "            line.axes.annotate('',\n",
    "                               xytext=(xdata[start_inds[i]], ydata[start_inds[i]]),\n",
    "                               xy=(xdata[end_inds[i]], ydata[end_inds[i]]),\n",
    "                               arrowprops=dict(arrowstyle=\"->\", color=color, linewidth=linewidth, alpha=alpha),\n",
    "                               size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_1 = eigv_time_mean[1]\n",
    "eig_2 = eigv_time_mean[2]\n",
    "eig_3 = eigv_time_mean[3]\n",
    "eig_4 = eigv_time_mean[4]\n",
    "eig_5 = eigv_time_mean[5]\n",
    "\n",
    "eig_1_nlsa = eigv_time_mean_nlsa[1]\n",
    "eig_2_nlsa = eigv_time_mean_nlsa[2]\n",
    "eig_3_nlsa = eigv_time_mean_nlsa[3]\n",
    "eig_4_nlsa = eigv_time_mean_nlsa[4]\n",
    "eig_5_nlsa = eigv_time_mean_nlsa[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save eigenvectors for geographical plots in other notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = [[0], [eig_1, eig_2], [eig_1, eig_3], [eig_1, eig_4], [eig_1, eig_5],\n",
    "         [eig_2, eig_1], [0], [eig_2, eig_3], [eig_2, eig_4], [eig_2, eig_5],\n",
    "         [eig_3, eig_1], [eig_3, eig_2], [0], [eig_3, eig_4], [eig_3, eig_5],\n",
    "         [eig_4, eig_1], [eig_4, eig_2], [eig_4, eig_3], [0], [eig_4, eig_5],\n",
    "         [eig_5, eig_1], [eig_5, eig_2], [eig_5, eig_3], [eig_5, eig_4], [0]]\n",
    "\n",
    "plots_nlsa = [[0], [eig_1_nlsa, eig_2_nlsa], [eig_1_nlsa, eig_3_nlsa],\n",
    "              [eig_1_nlsa, eig_4_nlsa], [eig_1_nlsa, eig_5_nlsa],\n",
    "              [eig_2_nlsa, eig_1_nlsa], [0], [eig_2_nlsa, eig_3_nlsa],\n",
    "              [eig_2_nlsa, eig_4_nlsa], [eig_2_nlsa, eig_5_nlsa],\n",
    "              [eig_3_nlsa, eig_1_nlsa], [eig_3_nlsa, eig_2_nlsa], [0],\n",
    "              [eig_3_nlsa, eig_4_nlsa], [eig_3_nlsa, eig_5_nlsa],\n",
    "              [eig_4_nlsa, eig_1_nlsa], [eig_4_nlsa, eig_2_nlsa],\n",
    "              [eig_4_nlsa, eig_3_nlsa], [0], [eig_4_nlsa, eig_5_nlsa],\n",
    "              [eig_5_nlsa, eig_1_nlsa], [eig_5_nlsa, eig_2_nlsa],\n",
    "              [eig_5_nlsa, eig_3_nlsa], [eig_5_nlsa, eig_4_nlsa], [0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplot with average trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplot(eigv_time,\n",
    "              plots,\n",
    "              dates_,\n",
    "              traj,\n",
    "              colors_lines,\n",
    "              colors_points,\n",
    "              winters_no_ssw=winters_no_ssw,\n",
    "              winters_ssw=winters_ssw):\n",
    "    fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "    m = 0\n",
    "    y_= {}\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            pl = plots[m]\n",
    "            if i != j:\n",
    "                #plot grey lines:\n",
    "                axs[i, j].plot(pl[0],\n",
    "                               pl[1],\n",
    "                               alpha=0.8,\n",
    "                               color='grey',\n",
    "                               linewidth=0.15)\n",
    "\n",
    "                # add special trajectories:\n",
    "                for y in y_:\n",
    "                    winter_year = pd.date_range(f'{y}-12', f'{y+1}-04')\n",
    "                    winter_year_eig_0 = pl[0][winter_year]\n",
    "                    winter_year_eig_1 = pl[1][winter_year]\n",
    "                    line = axs[i, j].plot(winter_year_eig_0,\n",
    "                                          winter_year_eig_1,\n",
    "                                          linewidth=1,\n",
    "                                          color=colors_lines[y_[y]],\n",
    "                                          label=f'{y}-{y+1}: {y_[y]}')[0]\n",
    "                    add_arrow(line)\n",
    "                \n",
    "                # add average ssw/no ssw trajectories:\n",
    "                eig1 = pl[0]\n",
    "                avg_ssw_winter_1, df = avg_traj_winter(eig1, traj_ssw_winter)\n",
    "                avg_ssw_D_winter_1, df = avg_traj_winter(eig1, traj_D_winter)\n",
    "                avg_ssw_S_winter_1, df = avg_traj_winter(eig1, traj_S_winter)\n",
    "                avg_no_ssw_winter_1, df = avg_traj_winter(eig1, traj_no_ssw_winter)\n",
    "\n",
    "                avg_ssw_spring_1, df = avg_traj_spring(eig1, traj_ssw_spring)\n",
    "                avg_ssw_D_spring_1, df = avg_traj_spring(eig1, traj_D_spring)\n",
    "                avg_ssw_S_spring_1, df = avg_traj_spring(eig1, traj_S_spring)\n",
    "                avg_no_ssw_spring_1, df = avg_traj_spring(eig1, traj_no_ssw_spring)\n",
    "                \n",
    "                eig2 = pl[1]\n",
    "                avg_ssw_winter_2,df = avg_traj_winter(eig2, traj_ssw_winter)\n",
    "                avg_ssw_D_winter_2, df = avg_traj_winter(eig2, traj_D_winter)\n",
    "                avg_ssw_S_winter_2, df = avg_traj_winter(eig2, traj_S_winter)\n",
    "                avg_no_ssw_winter_2, df = avg_traj_winter(eig2, traj_no_ssw_winter)\n",
    "\n",
    "                avg_ssw_spring_2, df = avg_traj_spring(eig2, traj_ssw_spring)\n",
    "                avg_ssw_D_spring_2, df = avg_traj_spring(eig2, traj_D_spring)\n",
    "                avg_ssw_S_spring_2, df = avg_traj_spring(eig2, traj_S_spring)\n",
    "                avg_no_ssw_spring_2, df = avg_traj_spring(eig2, traj_no_ssw_spring)\n",
    "                \n",
    "                line_no_ssw = axs[i, j].plot(avg_no_ssw_winter_1,\n",
    "                                          avg_no_ssw_winter_2,\n",
    "                                          linewidth=1,\n",
    "                                          color=colors_points['NO-SSW'],\n",
    "                                          label=f'NO-SSW')[0]\n",
    "                line_no_ssw = axs[i, j].plot(avg_no_ssw_spring_1,\n",
    "                                          avg_no_ssw_spring_2,\n",
    "                                          linewidth=1,\n",
    "                                          color=colors_points['NO-SSW'])[0]\n",
    "                \n",
    "                line_ssw = axs[i, j].plot(avg_ssw_winter_1,\n",
    "                                          avg_ssw_winter_2,\n",
    "                                          linewidth=1,\n",
    "                                          color=colors_points['SSW'],\n",
    "                                          label=f'SSW')[0]\n",
    "                \n",
    "                line_ssw = axs[i, j].plot(avg_ssw_D_winter_1,\n",
    "                                          avg_ssw_D_winter_2,\n",
    "                                          linewidth=1,\n",
    "                                          color=colors_points['D'],\n",
    "                                          label=f'D')[0]\n",
    "                \n",
    "                \n",
    "                line_ssw = axs[i, j].plot(avg_ssw_S_winter_1,\n",
    "                                          avg_ssw_S_winter_2,\n",
    "                                          linewidth=1,\n",
    "                                          color=colors_points['S'],\n",
    "                                          label=f'S')[0]\n",
    "                \n",
    "                line_ssw = axs[i, j].plot(avg_ssw_S_spring_1,\n",
    "                                          avg_ssw_S_spring_2,\n",
    "                                          linewidth=1,\n",
    "                                          color=colors_points['S'])[0]\n",
    "                \n",
    "                line_ssw = axs[i, j].plot(avg_ssw_D_spring_1,\n",
    "                                          avg_ssw_D_spring_2,\n",
    "                                          linewidth=1,\n",
    "                                          color=colors_points['D'])[0]\n",
    "                \n",
    "                                \n",
    "                line_ssw = axs[i, j].plot(avg_ssw_spring_1,\n",
    "                                          avg_ssw_spring_2,\n",
    "                                          linewidth=1,\n",
    "                                          color=colors_points['SSW'])[0] \n",
    "                for year in traj:\n",
    "                    if int(year) not in y_ and int(year) - 1 not in y_:\n",
    "                        for el in traj[year]:\n",
    "                            type_ = el[1]\n",
    "                            winter_year_eig_0 = pl[0][el[0]]\n",
    "                            winter_year_eig_1 = pl[1][el[0]]\n",
    "                            line = axs[i, j].plot(winter_year_eig_0,\n",
    "                                                  winter_year_eig_1,\n",
    "                                                  linewidth=0.8,\n",
    "                                                  color='purple',\n",
    "                                                  alpha=0.2)[0]\n",
    "                            add_arrow(line,\n",
    "                                      num_arrows=3,\n",
    "                                      linewidth=0.8,\n",
    "                                      alpha=0.2)\n",
    "                axs[i, j].locator_params(axis='x', nbins=3)\n",
    "            else:\n",
    "                # diagonal plots\n",
    "                winters_supp(eigv_time,\n",
    "                             axs[i, j],\n",
    "                             -1,\n",
    "                             i + 1,\n",
    "                             alpha=0.4,\n",
    "                             linewidth=1,\n",
    "                             traj=traj,\n",
    "                             colors_lines=colors_lines,\n",
    "                             winters_no_ssw=winters_no_ssw,\n",
    "                             winters_ssw=winters_ssw,\n",
    "                             colors=colors_points,y_ = y_,\n",
    "                             add_traj=True)\n",
    "            if i == 0:\n",
    "                axs[i, j].set_title(j + 1)\n",
    "            if j == 0:\n",
    "                axs[i, j].set_ylabel(i + 1)\n",
    "            if j == 3 and i == 2:\n",
    "                plt.rc('legend', fontsize=12)\n",
    "                axs[i, j].legend(loc='lower right')\n",
    "            m += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors_lines ={'S':'darkblue', 'D':'darkred', 'D+S':'darkorange', 'N':'green'}\n",
    "colors_points = {\n",
    "    'D': 'green',\n",
    "    'D_TM': 'darkgreen',\n",
    "    'S': 'deepskyblue',\n",
    "    'S_TM': 'darkblue', \n",
    "    'SSW':'red', \n",
    "    'NO-SSW': 'black'\n",
    "    \n",
    "}\n",
    "\n",
    "multiplot(eigv_time_mean_nlsa, plots_nlsa,dates_,traj,colors_lines, colors_points);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplot with all ssw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplot_allssw(eigv_time,\n",
    "              plots,\n",
    "              dates_,\n",
    "              traj,\n",
    "              colors_lines,\n",
    "              colors_points,\n",
    "              winters_no_ssw=winters_no_ssw,\n",
    "              winters_ssw=winters_ssw):\n",
    "    fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "    m = 0\n",
    "    y_= {}\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            pl = plots[m]\n",
    "            if i != j:\n",
    "                #plot grey lines:\n",
    "                axs[i, j].plot(pl[0],\n",
    "                               pl[1],\n",
    "                               alpha=0.8,\n",
    "                               color='grey',\n",
    "                               linewidth=0.15)\n",
    "                \n",
    "                \"\"\"# add average ssw/no ssw trajectories:\n",
    "                avg_no_ssw_0,df = average_traj(pl[0], winters_no_ssw)\n",
    "                avg_no_ssw_1,df = average_traj(pl[1], winters_no_ssw)\n",
    "                \n",
    "                avg_ssw_0,df = average_traj(pl[0], winters_ssw)\n",
    "                avg_ssw_1,df = average_traj(pl[1], winters_ssw)\n",
    "                \n",
    "                avg_ssw_0_D,df = average_traj(pl[0], winters_D)\n",
    "                avg_ssw_1_D,df = average_traj(pl[1], winters_D)\n",
    "                \n",
    "                avg_ssw_0_S,df = average_traj(pl[0], winters_S)\n",
    "                avg_ssw_1_S,df = average_traj(pl[1], winters_S)\n",
    "                \n",
    "                line_no_ssw = axs[i, j].plot(avg_no_ssw_0,\n",
    "                                          avg_no_ssw_1,\n",
    "                                          linewidth=1,\n",
    "                                          color=colors_points['NO-SSW'],\n",
    "                                          label=f'NO-SSW')[0]\n",
    "                add_arrow(line_no_ssw)\n",
    "                \n",
    "                line_ssw = axs[i, j].plot(avg_ssw_0,\n",
    "                                          avg_ssw_1,\n",
    "                                          linewidth=1,\n",
    "                                          color=colors_points['SSW'],\n",
    "                                          label=f'SSW')[0]\n",
    "                add_arrow(line_ssw)\n",
    "                line_ssw = axs[i, j].plot(avg_ssw_0_D,\n",
    "                                          avg_ssw_1_D,\n",
    "                                          linewidth=1,\n",
    "                                          color=colors_points['D'],\n",
    "                                          label=f'SSW-D')[0]\n",
    "                add_arrow(line_ssw)\n",
    "                line_ssw = axs[i, j].plot(avg_ssw_0_S,\n",
    "                                          avg_ssw_1_S,\n",
    "                                          linewidth=1,\n",
    "                                          color=colors_points['S'],\n",
    "                                          label=f'SSW-S')[0]\n",
    "                add_arrow(line_ssw)\"\"\"\n",
    "                \n",
    "                # add trajectories\n",
    "                for year in traj:\n",
    "                    if int(year) not in y_ and int(year) - 1 not in y_:\n",
    "                        for el in traj[year]:\n",
    "                            #type_ = el[1]\n",
    "                            type_ = 'D'\n",
    "                            winter_year_eig_0 = pl[0][el[0]]\n",
    "                            winter_year_eig_1 = pl[1][el[0]]\n",
    "                            line = axs[i, j].plot(winter_year_eig_0,\n",
    "                                                  winter_year_eig_1,\n",
    "                                                  linewidth=0.8,\n",
    "                                                  color=colors_points[type_],\n",
    "                                                  alpha=0.6)[0]\n",
    "                            add_arrow(line,\n",
    "                                      num_arrows=3,\n",
    "                                      linewidth=0.8,\n",
    "                                      alpha=0.5)\n",
    "                axs[i, j].locator_params(axis='x', nbins=3)\n",
    "\n",
    "            else:\n",
    "                # diagonal plots\n",
    "                winters_supp(eigv_time,\n",
    "                             axs[i, j],\n",
    "                             -1,\n",
    "                             i + 1,\n",
    "                             alpha=0.4,\n",
    "                             linewidth=1,\n",
    "                             traj=traj,\n",
    "                             colors_lines=colors_lines,\n",
    "                             winters_no_ssw=winters_no_ssw,\n",
    "                             winters_ssw=winters_ssw,\n",
    "                             colors=colors_points,y_ = y_,\n",
    "                             add_traj=True)\n",
    "            if i == 0:\n",
    "                axs[i, j].set_title(j + 1)\n",
    "            if j == 0:\n",
    "                axs[i, j].set_ylabel(i + 1)\n",
    "            if j == 3 and i == 2:\n",
    "                plt.rc('legend', fontsize=12)\n",
    "                axs[i, j].legend(loc='lower right')\n",
    "            m += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_lines ={'S':'darkblue', 'D':'darkred', 'D+S':'darkorange', 'N':'green'}\n",
    "colors_points = {\n",
    "    'D': 'purple',\n",
    "    'D_TM': 'darkred',\n",
    "    'S': 'deepskyblue',\n",
    "    'S_TM': 'darkblue', \n",
    "    'SSW':'red', \n",
    "    'NO-SSW': 'black'\n",
    "    \n",
    "}\n",
    "\n",
    "multiplot_allssw(eigv_time_mean_nlsa, plots_nlsa,dates_,traj,colors_lines, colors_points);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplot with clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = plots_nlsa[1][0].loc[jan_feb['cluster_label_spectral_kernel'].index]\n",
    "\n",
    "clusters = jan_feb['cluster_label_spectral_kernel'].loc[time_series.index].values\n",
    "LABEL_COLOR_MAP = {0 : 'red',\n",
    "                   1 : 'green',\n",
    "                   2:'blue',\n",
    "                   }\n",
    "\n",
    "label_color = [LABEL_COLOR_MAP[l] for l in clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplot_clusters(eigv_time,\n",
    "                       plots,\n",
    "                       dates_,\n",
    "                       traj,\n",
    "                       colors_lines,\n",
    "                       colors_points,\n",
    "                       winters_no_ssw=winters_no_ssw,\n",
    "                       winters_ssw=winters_ssw):\n",
    "    fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "    m = 0\n",
    "    y_ = {}\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            pl = plots[m]\n",
    "            if i != j:\n",
    "                #plot grey lines:\n",
    "                axs[i, j].plot(pl[0],\n",
    "                               pl[1],\n",
    "                               alpha=0.8,\n",
    "                               color='grey',\n",
    "                               linewidth=0.15)\n",
    "\n",
    "                time_series1 = pl[0].loc[jan_feb['cluster_label_spectral_kernel'].index]\n",
    "                time_series2 = pl[1].loc[jan_feb['cluster_label_spectral_kernel'].index]\n",
    "                clusters = jan_feb['cluster_label_spectral_kernel'].loc[\n",
    "                    time_series.index].values\n",
    "                LABEL_COLOR_MAP = {\n",
    "                    0: 'red',\n",
    "                    1: 'green',\n",
    "                    2: 'blue',\n",
    "                }\n",
    "\n",
    "                label_color = [LABEL_COLOR_MAP[l] for l in clusters]\n",
    "                axs[i, j].scatter(time_series1,\n",
    "                                  time_series2,\n",
    "                                  alpha=0.3,\n",
    "                                  c=label_color,\n",
    "                                  s=2,\n",
    "                                  linewidth=0.15)\n",
    "\n",
    "                # add special trajectories:\n",
    "                for y in y_:\n",
    "                    winter_year = pd.date_range(f'{y}-12', f'{y+1}-04')\n",
    "                    winter_year_eig_0 = pl[0][winter_year]\n",
    "                    winter_year_eig_1 = pl[1][winter_year]\n",
    "                    line = axs[i, j].plot(winter_year_eig_0,\n",
    "                                          winter_year_eig_1,\n",
    "                                          linewidth=1,\n",
    "                                          color=colors_lines[y_[y]],\n",
    "                                          label=f'{y}-{y+1}: {y_[y]}')[0]\n",
    "                    add_arrow(line)\n",
    "\n",
    "                for year in traj:\n",
    "                    if int(year) not in y_ and int(year) - 1 not in y_:\n",
    "                        for el in traj[year]:\n",
    "                            type_ = el[1]\n",
    "                            winter_year_eig_0 = pl[0][el[0]]\n",
    "                            winter_year_eig_1 = pl[1][el[0]]\n",
    "                            line = axs[i, j].plot(winter_year_eig_0,\n",
    "                                                  winter_year_eig_1,\n",
    "                                                  linewidth=0.8,\n",
    "                                                  color='purple',\n",
    "                                                  alpha=0.2)[0]\n",
    "                            add_arrow(line,\n",
    "                                      num_arrows=3,\n",
    "                                      linewidth=0.8,\n",
    "                                      alpha=0.2)\n",
    "                axs[i, j].locator_params(axis='x', nbins=3)\n",
    "\n",
    "            else:\n",
    "                # diagonal plots\n",
    "                winters_supp(eigv_time,\n",
    "                             axs[i, j],\n",
    "                             -1,\n",
    "                             i + 1,\n",
    "                             alpha=0.4,\n",
    "                             linewidth=1,\n",
    "                             traj=traj,\n",
    "                             colors_lines=colors_lines,\n",
    "                             winters_no_ssw=winters_no_ssw,\n",
    "                             winters_ssw=winters_ssw,\n",
    "                             colors=colors_points,\n",
    "                             y_=y_,\n",
    "                             add_traj=False)\n",
    "            if i == 0:\n",
    "                axs[i, j].set_title(j + 1)\n",
    "            if j == 0:\n",
    "                axs[i, j].set_ylabel(i + 1)\n",
    "            if j == 3 and i == 2:\n",
    "                plt.rc('legend', fontsize=12)\n",
    "                axs[i, j].legend(loc='lower right')\n",
    "            m += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplot_clusters(eigv_time_mean_nlsa, plots_nlsa, dates_, traj, colors_lines,\n",
    "                   colors_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shows that there is some structure, canât say what without doing the spacial reconstruction, here have nonlinear dependance\n",
    "look at spatial pattern of third eigenvector of anomalies, circles capture some rotation \n",
    "try to understand the states of the vortex and is there a clear classification between those states\n",
    "read up on lores attractor, represent climatical systems with 3d summaries looking like some buterlfly\n",
    "taking 1 versus 2 we can define three regions, select times which correspond to theses regions and taking these samples in the original space we compute means if these samples and plot in original space â> give average pattern which corresponds to data\n",
    "pattern 2 versus 5 looks best to define about three regions\n",
    "\n",
    "for anomalies, can take just average in quadrants of outer regions for the same kind of plot\n",
    "\n",
    "for diagonal of anomalies, take threshold and take time steps above threshold and look at what average of those look like\n",
    "\n",
    "tau  = 1 month (30 days) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ampl(eig_1,eig_2):\n",
    "    r = np.sqrt(np.real(eig_1.values)**2 + np.real(eig_2.values**2))\n",
    "    amplitude = pd.Series(r, index = eig_1.index)\n",
    "    return amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot overlapping amplitudes:\n",
    "def ampl_supp(eig1,\n",
    "              eig2,\n",
    "              ax,\n",
    "              sign,\n",
    "              traj,\n",
    "              colors_lines=colors_lines,\n",
    "              colors=colors_points,\n",
    "              alpha=0.5,\n",
    "              linewidth=1,\n",
    "              add_traj=False,\n",
    "              y_={\n",
    "                  2010: 'D',\n",
    "                  1998: 'D+S',\n",
    "                  2008: 'S',\n",
    "                  1990: 'N'\n",
    "              }):\n",
    "    amplitude = calculate_ampl(eig1, eig2)\n",
    "    years = amplitude.index.year.unique()\n",
    "    n = len(years)\n",
    "    i = 1\n",
    "    for year in years[:-1]:\n",
    "        winter_of_that_year = pd.date_range(str(year) + '-12',\n",
    "                                            str(year + 1) + '-04',\n",
    "                                            freq='6H')\n",
    "        winter_ampl = amplitude[winter_of_that_year]\n",
    "        ax.plot(transform_date(winter_ampl.index),\n",
    "                sign * winter_ampl.values,\n",
    "                color='grey',\n",
    "                alpha=alpha,\n",
    "                linewidth=linewidth)\n",
    "\n",
    "        # Add dates:\n",
    "        if str(year) in years_in_d:\n",
    "            for el in years_in_d[str(year)]:\n",
    "                d_ = pd.to_datetime(el[0])\n",
    "                if d_.month >= 12:\n",
    "                    ax.plot(transform_d(d_, 1983),\n",
    "                            sign * winter_ampl[d_],\n",
    "                            'x',\n",
    "                            color=colors[el[1]])\n",
    "        if str(year + 1) in years_in_d:\n",
    "            for el in years_in_d[str(year + 1)]:\n",
    "                d_ = pd.to_datetime(el[0])\n",
    "                if d_.month < 4:\n",
    "                    ax.plot(transform_d(d_, 1984),\n",
    "                            sign * winter_ampl[d_],\n",
    "                            'x',\n",
    "                            color=colors[el[1]])\n",
    "        # special trajectories:\n",
    "        if year in y_:\n",
    "            ax.plot(transform_date(winter_ampl.index),\n",
    "                    sign * winter_ampl.values,\n",
    "                    color=colors_lines[y_[year]],\n",
    "                    alpha=0.8,\n",
    "                    linewidth=linewidth,\n",
    "                    label=f'{year}-{year+1}: {y_[year]}')\n",
    "\n",
    "        if add_traj:\n",
    "            if str(year) in traj and year not in y_:\n",
    "                for el in traj[str(year)]:\n",
    "                    trajectory = amplitude[el[0]]\n",
    "                    type_ = el[1]\n",
    "                    ax.plot(transform_date(trajectory.index,\n",
    "                                           special_traj=True),\n",
    "                            sign * trajectory.values,\n",
    "                            color=colors[type_],\n",
    "                            alpha=0.5,\n",
    "                            linewidth=0.8)\n",
    "\n",
    "        if year == 1983:\n",
    "            ax.set_xticks(\n",
    "                ('1983-12', '1984-01', '1984-02', '1984-03', '1984-04'))\n",
    "            ax.set_xticklabels(['Dec', 'Jan', 'Feb', 'Mar', 'Apr'])\n",
    "        i += 1\n",
    "    plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ampl_supp(eigv_time_mean_nlsa[1],eigv_time_mean_nlsa[2], ax, 1, traj = traj, add_traj =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplot_amplitude(eigv_time, plots, dates_, traj, colors_lines,\n",
    "                        colors_points):\n",
    "    fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "    m = 0\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "\n",
    "            #y_= {2010:'D', 1998:'D+S', 2008:'S', 1990:'N'}\n",
    "            y_ = {1990: 'N'}\n",
    "            pl = plots[m]\n",
    "            if j > i:\n",
    "                amplitude = calculate_ampl(pl[0], pl[1])\n",
    "                ampl_supp(pl[0],\n",
    "                          pl[1],\n",
    "                          axs[i, j],\n",
    "                          1,\n",
    "                          alpha=0.4,\n",
    "                          linewidth=1,\n",
    "                          colors_lines=colors_lines,\n",
    "                          colors=colors_points,\n",
    "                          traj=traj,\n",
    "                          add_traj=True,\n",
    "                          y_=y_)\n",
    "                axs[i, j].locator_params(axis='x', nbins=3)\n",
    "\n",
    "            if j == i:\n",
    "                # diagonal plots\n",
    "                winters_supp(eigv_time,\n",
    "                             axs[i, j],\n",
    "                             -1,\n",
    "                             i + 1,\n",
    "                             alpha=0.4,\n",
    "                             linewidth=1,\n",
    "                             traj=traj,\n",
    "                             colors_lines=colors_lines,\n",
    "                             colors=colors_points,\n",
    "                             add_traj=True,\n",
    "                             y_=y_)\n",
    "            if j < i:\n",
    "                #plot grey lines:\n",
    "                axs[i, j].plot(pl[0],\n",
    "                               pl[1],\n",
    "                               alpha=0.8,\n",
    "                               color='grey',\n",
    "                               linewidth=0.15)\n",
    "                # add special dates:\n",
    "                for el in dates_[1:]:\n",
    "                    d_ = el[0]\n",
    "                    axs[i, j].plot(pl[0][d_],\n",
    "                                   pl[1][d_],\n",
    "                                   'x',\n",
    "                                   color=colors_points[el[1]])\n",
    "                # add special trajectories:\n",
    "                for y in y_:\n",
    "                    winter_year = pd.date_range(f'{y}-12', f'{y+1}-04')\n",
    "                    winter_year_eig_0 = pl[0][winter_year]\n",
    "                    winter_year_eig_1 = pl[1][winter_year]\n",
    "                    line = axs[i, j].plot(winter_year_eig_0,\n",
    "                                          winter_year_eig_1,\n",
    "                                          linewidth=1,\n",
    "                                          color=colors_lines[y_[y]],\n",
    "                                          label=f'{y}-{y+1}: {y_[y]}')[0]\n",
    "                    add_arrow(line)\n",
    "                for year in traj:\n",
    "                    if int(year) not in y_ and int(year) - 1 not in y_:\n",
    "                        for el in traj[year]:\n",
    "                            type_ = el[1]\n",
    "                            winter_year_eig_0 = pl[0][el[0]]\n",
    "                            winter_year_eig_1 = pl[1][el[0]]\n",
    "                            line = axs[i, j].plot(winter_year_eig_0,\n",
    "                                                  winter_year_eig_1,\n",
    "                                                  linewidth=0.8,\n",
    "                                                  color=colors_points[type_],\n",
    "                                                  alpha=0.7)[0]\n",
    "                            add_arrow(line,\n",
    "                                      num_arrows=3,\n",
    "                                      linewidth=0.7,\n",
    "                                      alpha=0.8)\n",
    "                axs[i, j].locator_params(axis='x', nbins=3)\n",
    "            if i == 0:\n",
    "                axs[i, j].set_title(j + 1)\n",
    "            if j == 0:\n",
    "                axs[i, j].set_ylabel(i + 1)\n",
    "            m += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "multiplot_amplitude(eigv_time_mean_nlsa, plots_nlsa, dates_, traj, colors_lines,\n",
    "                    colors_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Spectrums:\n",
    "Power spectrum plot for all years and then see something every 7 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate_seconds = 4 / (24 * 3600)\n",
    "print(f'Frequency: {sampling_rate_seconds} [s-1]')\n",
    "sampling_rate_days = 4\n",
    "print(f'Frequency: {sampling_rate_days} [d-1]')\n",
    "sampling_time = 1 / sampling_rate_days\n",
    "print(f'Sampling every  {sampling_time*24} hours')\n",
    "year_t = 365\n",
    "two_year_t = (365 / 2)\n",
    "three_year_t = (365 / 3)\n",
    "freq_year = 1 / year_t\n",
    "freq_two_year = 1 / two_year_t\n",
    "freq_three_year = 1 / three_year_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pperations on frequency : log(frequency) -> in time 1/frequency\n",
    "indicate the 1/year, 2/year and 3/year frequencie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate power spectral density using a periodogram. Change to 1/frequency to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Power spectrum with zero padding for the missing dates:\n",
    "\"\"\"\n",
    "def pwr_compl(eigv_time, sampling_rate_days, number, axs):\n",
    "    full_time = pd.date_range(start='1/1/1979', end='1/1/2019', freq='6H')\n",
    "    full_df = pd.DataFrame(np.zeros((len(full_time), 20)))\n",
    "    eigv_time = eigv_time.copy()\n",
    "    full_df.index = full_time\n",
    "    full_df.columns = [str(i) for i in eigv_time.columns]\n",
    "    eigv_time.columns = [str(i) for i in eigv_time.columns]\n",
    "\n",
    "    cols_to_use = full_df.transpose().columns.difference(\n",
    "        eigv_time.transpose().columns)\n",
    "    full_df_time = pd.merge(left=full_df.transpose()[cols_to_use],\n",
    "                            right=eigv_time.transpose(),\n",
    "                            how='right',\n",
    "                            left_index=True,\n",
    "                            right_index=True,\n",
    "                            suffixes='_x').transpose()\n",
    "    full_df_time = full_df_time.sort_index()\n",
    "    f, Pxx_den = signal.periodogram(full_df_time[str(number)], sampling_rate_days)\n",
    "    axs.semilogy(np.log(1 / f), Pxx_den)\n",
    "    axs.axvline(np.log(year_t), color='r', linestyle=':', label='Every year')\n",
    "    axs.axvline(np.log(two_year_t),\n",
    "                color='g',\n",
    "                linestyle=':',\n",
    "                label='Twice per year')\n",
    "    axs.axvline(np.log(three_year_t),\n",
    "                color='orange',\n",
    "                linestyle=':',\n",
    "                label='Thrice per year')\n",
    "    #axs.set_title(\"Powerspectrum of first eigenvector of Heat kernel\")\n",
    "    axs.set_xticklabels([])\n",
    "    #axs.set_xlabel('Days')\n",
    "    #axs.set_ylabel('log(time [d])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA == 'raw':\n",
    "    sign_mean = [0, 1, 1, -1, -1,1,1,1,1,1,1,1, 1, -1, -1,1,1,1,1,1,1]\n",
    "    sign_simple = [0, 1, 1, -1, -1,-1,1,-1,1,1,1,1, 1, -1, -1,1,1,1,1,1,1]\n",
    "    if USE_TAKENS:\n",
    "        sign_mean = [0, 1, 1, -1, -1,1,1,1,1,1,1,1, 1, -1, -1,1,1,1,1,1,1]\n",
    "        sign_simple = [0, 1, 1, 1, -1,-1,-1,1,-1,1,-1,-1, 1, 1, 1,-1,-1,1,1,1,1]\n",
    "else:\n",
    "    sign_mean = [0, 1, 1, -1, -1,1,1,1,1,1,1,1, 1, -1, -1,1,1,1,1,1,1]\n",
    "    sign_simple = [0, 1, 1, 1, -1,1,1,1,-1,1,1,1, 1, 1, 1,1,1,-1,1,-1,-1]\n",
    "    if USE_TAKENS:\n",
    "        sign_mean = [0, 1, 1, -1, -1,1,1,1,1,1,1,1, 1, -1, -1,1,1,1,1,1,1]\n",
    "        sign_simple = [0, -1, 1, 1, -1,-1,1,1,1,1,-1,1, 1, -1, 1,-1,1,-1,-1,1,1]\n",
    "eigv_time_mean_nlsa =   eigv_time_mean_nlsa.drop('cluster_label_spectral_kernel', axis = 1) \n",
    "fig, axs = plt.subplots(10, 4, figsize=(30, 35))\n",
    "m = 1\n",
    "n = 1\n",
    "for j in range(4):\n",
    "    for i in range(10):\n",
    "        if j==0 or j==2:\n",
    "            winter_2008 = pd.date_range('2008-12', '2009-04')\n",
    "            winter_2008_eig_mean = eigv_time_mean_nlsa[n][winter_2008]\n",
    "\n",
    "            axs[i, j].plot(winter_2008_eig_mean.index,\n",
    "                           sign_mean[n] * winter_2008_eig_mean.values,\n",
    "                           label='heat kernel')\n",
    "\n",
    "            axs[i, j].set_xticklabels([])\n",
    "            axs[i, j].set_ylabel(f\"EigenvectorÂ {n}\")\n",
    "            if i==9:\n",
    "                #axs[i, j].set_xticklabels(['Nov', 'Dec', 'Jan', 'Feb', 'Mar', 'Ap'])\n",
    "                \n",
    "                axs[i, j].set_xticks(\n",
    "                ('2008'+ '-12', '2009' + '-01', '2009' + '-02',\n",
    "                 '2009' + '-03', '2009' + '-04'))\n",
    "                axs[i, j].set_xticklabels(['Dec', 'Jan', 'Feb', 'Mar', 'Apr'])\n",
    "                axs[i,j].set_xlabel('Months')\n",
    "            n += 1\n",
    "        else:   \n",
    "            pwr_compl(eigv_time_mean_nlsa, sampling_rate_days, m, axs[i,j])\n",
    "            m+= 1\n",
    "            if i==9:\n",
    "                axs[i,j].set_xticklabels(['0', '1', '7', '54', '403', '2980'])\n",
    "                axs[i,j].set_xlabel('Days')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection on geographical data: \n",
    "Perform projection $X'\\in R^{1001x1}= X^T D \\phi_i$. $X'_i = X D\\phi_i \\phi_i^T, \\phi \\in R^{33960}, X \\in R^{Nx1001}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chosen_t = 'mean'\n",
    "if DATA == 'anomalies':\n",
    "    if USE_TAKENS:\n",
    "        print(f'Reading {DATA} data from {INPUT_DATA}')\n",
    "        df = pd.read_csv(INPUT_DATA + 'anomalies_coefficients.csv', sep=',')\n",
    "        print(f'Reading eigenvectors from {PATH1}')\n",
    "        eigv = pd.read_pickle(PATH1 + 't_' + chosen_t +\n",
    "                              '/takens/eigenvectors_heat_matrix_t_' +\n",
    "                              chosen_t + '_takens_.pkl').values\n",
    "        eigv = pd.DataFrame(eigv).drop([0], axis=1).values\n",
    "\n",
    "    else:\n",
    "        print(f'Reading {DATA} data from {INPUT_DATA}')\n",
    "        df = pd.read_csv(INPUT_DATA + 'anomalies_coefficients.csv', sep=',')\n",
    "        print(f'Reading eigenvectors from {PATH1}')\n",
    "        eigv = pd.read_pickle(\n",
    "            PATH1 + 't_mean/eigenvectors_heat_matrix_t_mean_.pkl').values\n",
    "        eigv = pd.DataFrame(eigv).drop([0], axis=1).values\n",
    "\n",
    "else:\n",
    "    if USE_TAKENS:\n",
    "        print(f'Reading {DATA} data from {INPUT_DATA}')\n",
    "        df = pd.read_csv(INPUT_DATA + 'raw_data_coefficients.csv', sep=',')\n",
    "        print(f'Reading eigenvectors from {PATH1}')\n",
    "        eigv = pd.read_pickle(PATH1 + 't_' + chosen_t +\n",
    "                              '/takens/eigenvectors_heat_matrix_t_' +\n",
    "                              chosen_t + '_takens_.pkl').values\n",
    "        eigv = pd.DataFrame(eigv).drop([1], axis=1).values\n",
    "    else:\n",
    "        print(f'Reading {DATA} data from {INPUT_DATA}')\n",
    "        df = pd.read_csv(INPUT_DATA + 'raw_data_coefficients.csv', sep=',')\n",
    "        print(f'Reading eigenvectors from {PATH1}')\n",
    "        eigv = pd.read_pickle(\n",
    "            PATH1 + 't_mean/eigenvectors_heat_matrix_t_mean_.pkl').values\n",
    "        eigv = pd.DataFrame(eigv).drop([1], axis=1).values\n",
    "\n",
    "print(f'Reading diagonal matrix from {PATH1}')\n",
    "if USE_TAKENS:\n",
    "    D = pd.read_pickle(PATH1 +\n",
    "                       't_mean/takens/diagonal_heat_matrix_t_mean_takens_.pkl').values\n",
    "    X = df.drop(['Unnamed: 0', 'Date'], axis=1)\n",
    "    X_nlsa = X.loc[indices_of_points[0]]\n",
    "    for i in range(1, len(last_pos_winters)):\n",
    "        X_nlsa = pd.concat([X_nlsa, X.loc[indices_of_points[i]]], axis = 0)\n",
    "    Xt = X_nlsa.values.transpose()\n",
    "else:\n",
    "    D = pd.read_pickle(PATH1 +\n",
    "                       't_mean/diagonal_heat_matrix_t_mean_.pkl').values\n",
    "\n",
    "    Xt = df.drop(['Unnamed: 0', 'Date'], axis=1).values.transpose()\n",
    "\n",
    "print(f'Xt shape: {Xt.shape}')\n",
    "print(f'Diagonal matrix shape: {D.shape}')\n",
    "print(f'Eigenvectors shape: {eigv.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projections on eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUM_EIGENVALUES-1):\n",
    "    X_prime_i = Xt @ D @  eigv[:,i]\n",
    "    np.save(PATH1 +'t_mean/takens/'+ 'X_prime_{}_takens_.npy'.format(i), X_prime_i)\n",
    "    print('Saving X_prime_{}_takens_.npy at {}'.format(i, PATH1+'t_mean/takens/'))\n",
    "del Xt, D, eigv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments: \n",
    "- First request a lot of eigenvectors and check that only one eigenvector that has eigenvalue close to zero (~$10^{-15}$) and might need to increase number of neighbours otherwise (check if graph is connected): **DONE FOR 50 EIGENVALUES, ONLY ONE ZERO**\n",
    "- Take the mean over all winters : **DONE**\n",
    "- Change to heat kernel and compute for different kernel values, and more than 10 eigenvectors. Plot also the eigenvalues. **DONE**\n",
    "- To choose t: look at distances and take mean or max of all distances among neighbours. Check if t is not the squared of distance. For large t we should have something close to what we have now. \n",
    "- Move to anomaly data\n",
    "- Power spectrum with missing data ? Compute power spectrum for other eigenvectors. So up to 7 months alright and then a gap and the rest should start at 360 and multiply by $365/(7x30)$ **DONE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- anomaly data estimated seasonality from raw data. estimate smooth mean and remove it to obtain the anomaly. \n",
    "- switch from default weight to gaussian kernel, optimise bandwidth computational demanding, try multiple experiments, assess the best bandwidth also define what best means\n",
    "- apply also to anomaly data to see if consistent\n",
    "- include to the model so that it can capture the dynamics of system, those things are called backends embedding (use multiple time steps to represent one sample -> sequences) â> apply and optimite previous all to new representation of data \n",
    "\n",
    "Pic raw data so that the first mode of variability should be the seasonality \n",
    "capturing something, huge variability but donât know what\n",
    "\n",
    "gaussian kernel approximates distance on manifold using this laplacian graph. The 0-1 weights is not very realistic because two samples that might be similar they might be different, For now they all have the sae weight. Not very good to approximate distances. For bandwidth t goes to 0 and a lot of data, converge to the distance on the manifold. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "229.422px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
