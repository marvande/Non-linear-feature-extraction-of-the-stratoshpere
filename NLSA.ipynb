{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non linear feature extraction of the stratosphere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import warnings\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import scipy.sparse.linalg as linalg\n",
    "import scipy.signal as signal\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import os\n",
    "import math \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from functions_takens import *\n",
    "\n",
    "%load_ext jupyternotify\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of nearest neighbours: 0.1\n",
      "Number of eigenmaps: 21\n",
      "Data set considered: anomalies\n",
      "Path to input data: ../../../data/vandermeer/input_data/\n",
      "Using takens embedding: True\n",
      "tau = 2.0 months\n"
     ]
    }
   ],
   "source": [
    "# Percentage of nearest neighbours for Laplacian eigenmpas:\n",
    "PERC_NEIGH = 10\n",
    "print(f'Percentage of nearest neighbours: {PERC_NEIGH/100}')\n",
    "\n",
    "# Number of eigenmaps to compute:\n",
    "NUM_EIGENVALUES = 21\n",
    "print(f'Number of eigenmaps: {NUM_EIGENVALUES}')\n",
    "\n",
    "# Data set to consider: ('raw/anomalies')\n",
    "DATA = 'anomalies'\n",
    "print(f'Data set considered: {DATA}')\n",
    "\n",
    "# Path to input data:\n",
    "INPUT_DATA = '../../../data/vandermeer/input_data/'\n",
    "print(f'Path to input data: {INPUT_DATA}')\n",
    "\n",
    "# Wether to do NLSA or Laplacian Eigenmaps. Takens True for NLSA\n",
    "USE_TAKENS = True\n",
    "print(f'Using takens embedding: {USE_TAKENS}')\n",
    "\n",
    "# Time-step in Takens embedding:\n",
    "TAU = 4 * 30 * 2\n",
    "print(f'tau = {TAU/(4*30)} months')\n",
    "\n",
    "BEGIN_YEAR = 1979\n",
    "END_YEAR = 2019\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for plots:\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 14\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)  # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)  # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=BIGGER_SIZE)  # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Paths:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global path to save data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global path: ../../../data/vandermeer/pickles/anomalies/\n"
     ]
    }
   ],
   "source": [
    "if DATA == 'raw':\n",
    "    PATH = '../../../data/vandermeer/pickles/raw/'\n",
    "elif DATA == 'anomalies':\n",
    "    PATH = '../../../data/vandermeer/pickles/anomalies/'\n",
    "if not os.path.exists(PATH):\n",
    "    os.makedirs(PATH)\n",
    "print(f'Global path: {PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to save data according to the percentage of neighbours used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precise path: ../../../data/vandermeer/pickles/anomalies/10perc/\n"
     ]
    }
   ],
   "source": [
    "PATH1 = PATH + str(PERC_NEIGH)+'perc/'\n",
    "if not os.path.exists(PATH1):\n",
    "        os.makedirs(PATH1)\n",
    "print(f'Precise path: {PATH1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to save data when using simple kernel (binary kernel): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to simple kernel: ../../../data/vandermeer/pickles/anomalies/10perc/simple_kernel/\n",
      "Path to simple kernel for NLSA with Takens embedding: ../../../data/vandermeer/pickles/anomalies/10perc/simple_kernel/takens/\n"
     ]
    }
   ],
   "source": [
    "# path to simple_kernel:\n",
    "# without Takens:\n",
    "PATH_SIMPLE = PATH1 + 'simple_kernel/'\n",
    "if not os.path.exists(PATH_SIMPLE):\n",
    "        os.makedirs(PATH_SIMPLE)\n",
    "print(f'Path to simple kernel: {PATH_SIMPLE}')\n",
    "\n",
    "# with Takens:\n",
    "PATH_SIMPLE_TAKENS = PATH1 + 'simple_kernel/takens/'\n",
    "if not os.path.exists(PATH_SIMPLE_TAKENS):\n",
    "        os.makedirs(PATH_SIMPLE_TAKENS)\n",
    "print(f'Path to simple kernel for NLSA with Takens embedding: {PATH_SIMPLE_TAKENS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to save results of NLSA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to NLSA results: ../../../data/vandermeer/pickles/anomalies/10perc/takens/\n"
     ]
    }
   ],
   "source": [
    "# path to NLSA results:\n",
    "PATH_TAKENS = PATH + str(PERC_NEIGH)+'perc/takens/'\n",
    "if not os.path.exists(PATH_TAKENS):\n",
    "        os.makedirs(PATH_TAKENS)\n",
    "print(f'Path to NLSA results: {PATH_TAKENS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data:\n",
    "\n",
    "Load input raw or anomalies data and basis coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of data:\n",
      "Shape of anomalies data: (33960, 1001)\n",
      "CPU times: user 21.3 s, sys: 1.37 s, total: 22.7 s\n",
      "Wall time: 22.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X992</th>\n",
       "      <th>X993</th>\n",
       "      <th>X994</th>\n",
       "      <th>X995</th>\n",
       "      <th>X996</th>\n",
       "      <th>X997</th>\n",
       "      <th>X998</th>\n",
       "      <th>X999</th>\n",
       "      <th>X1000</th>\n",
       "      <th>X1001</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.087772</td>\n",
       "      <td>0.306971</td>\n",
       "      <td>-0.358278</td>\n",
       "      <td>-0.713848</td>\n",
       "      <td>0.431876</td>\n",
       "      <td>0.233288</td>\n",
       "      <td>0.079006</td>\n",
       "      <td>0.182841</td>\n",
       "      <td>0.420922</td>\n",
       "      <td>0.575587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010085</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.013018</td>\n",
       "      <td>-0.008956</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>-0.000141</td>\n",
       "      <td>0.009586</td>\n",
       "      <td>-0.004209</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.011727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.045790</td>\n",
       "      <td>0.298297</td>\n",
       "      <td>-0.373760</td>\n",
       "      <td>-0.684706</td>\n",
       "      <td>0.456666</td>\n",
       "      <td>0.171122</td>\n",
       "      <td>0.098944</td>\n",
       "      <td>0.153985</td>\n",
       "      <td>0.360332</td>\n",
       "      <td>0.543631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004224</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>0.003769</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>-0.001214</td>\n",
       "      <td>0.010168</td>\n",
       "      <td>-0.005202</td>\n",
       "      <td>0.007966</td>\n",
       "      <td>0.002296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.044001</td>\n",
       "      <td>0.297177</td>\n",
       "      <td>-0.449389</td>\n",
       "      <td>-0.679847</td>\n",
       "      <td>0.513206</td>\n",
       "      <td>0.161827</td>\n",
       "      <td>0.082390</td>\n",
       "      <td>0.152011</td>\n",
       "      <td>0.281488</td>\n",
       "      <td>0.510341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>-0.010951</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>-0.004194</td>\n",
       "      <td>-0.000835</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.005812</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>-0.005325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0  0.087772  0.306971 -0.358278 -0.713848  0.431876  0.233288  0.079006   \n",
       "1  0.045790  0.298297 -0.373760 -0.684706  0.456666  0.171122  0.098944   \n",
       "2  0.044001  0.297177 -0.449389 -0.679847  0.513206  0.161827  0.082390   \n",
       "\n",
       "         X8        X9       X10  ...      X992      X993      X994      X995  \\\n",
       "0  0.182841  0.420922  0.575587  ...  0.010085  0.000343  0.013018 -0.008956   \n",
       "1  0.153985  0.360332  0.543631  ...  0.004224 -0.002052  0.002568  0.003769   \n",
       "2  0.152011  0.281488  0.510341  ...  0.009204 -0.010951  0.000831 -0.004194   \n",
       "\n",
       "       X996      X997      X998      X999     X1000     X1001  \n",
       "0  0.001028 -0.000141  0.009586 -0.004209  0.000906  0.011727  \n",
       "1  0.000829 -0.001214  0.010168 -0.005202  0.007966  0.002296  \n",
       "2 -0.000835  0.001100 -0.000148 -0.005812  0.000562 -0.005325  \n",
       "\n",
       "[3 rows x 1001 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "anomalies_cf = pd.read_csv(INPUT_DATA + 'anomalies_coefficients.csv', sep=',')\n",
    "raw_cf = pd.read_csv(INPUT_DATA + 'raw_data_coefficients.csv', sep=',')\n",
    "basis_cf = pd.read_csv(INPUT_DATA + 'basis_functions.csv', sep=',')\n",
    "\n",
    "if DATA == 'raw':\n",
    "    df = raw_cf\n",
    "elif DATA == 'anomalies':\n",
    "    df = anomalies_cf\n",
    "\n",
    "# remove useless axes:\n",
    "df = df.drop(['Unnamed: 0', 'Date'], axis=1)\n",
    "print('Sample of data:')\n",
    "print(f'Shape of {DATA} data: {df.shape}')\n",
    "pd.DataFrame(df).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takens through takens matrix:\n",
    "\n",
    "Create the Takens embedding matrix directly from the data matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data matrix:\n",
    "if DATA == 'raw':\n",
    "    df = pd.read_csv(INPUT_DATA + 'raw_data_coefficients.csv', sep=',')\n",
    "elif DATA == 'anomalies':\n",
    "    df = pd.read_csv(INPUT_DATA + 'anomalies_coefficients.csv', sep=',')\n",
    "print(f'Input data shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Takens matrix:\n",
    "\n",
    "From $X$ the data matrix (raw or anomalies) we build the Takens embedding matrix as the following: \n",
    "\n",
    "$Y = X[1 : (T -m*\\tau); 1 : (Dx\\tau)]$ where $Y [t; 1 : D] = X[t; 1 : D]$ and $Y [1; (D + 1) : (2D)] = X[t + 1; 1 : D]$,...\n",
    "and where $m$ is the number of entire winters, $T$ the number of time-samples and $D$ the dimension of the original data matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with time as index and remove useless columns\n",
    "time = df['Date']\n",
    "time = pd.to_datetime(time)\n",
    "df_ = df.drop(['Unnamed: 0', 'Date'], axis=1)\n",
    "df_time = pd.concat([time, df_], axis=1)\n",
    "df_time = df_time.set_index('Date')\n",
    "\n",
    "# Years in the data:\n",
    "years = range(BEGIN_YEAR, END_YEAR)\n",
    "years = [str(y) for y in years]\n",
    "\n",
    "# Count number of samples per year:\n",
    "counts = {}\n",
    "sum_ = 0\n",
    "for y in years:\n",
    "    sum_ += len(df_time[y])\n",
    "    counts[y] = len(df_time[y])\n",
    "print('For example, number of samples in 1979 and 1980: {}, {}'.format(counts['1979'], counts['1980']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the position of the last element for each year \n",
    "cs = [counts[i] for i in years]\n",
    "last_year_pos = [np.sum(cs[:i + 1]) for i in range(len(cs))]\n",
    "\n",
    "# get the number of entire winters e.g, winters that range \n",
    "# from Dec - April (not like 1979 and 2019)\n",
    "entire_winters = END_YEAR - BEGIN_YEAR - 1\n",
    "print(f'Number of entire winters in data: {entire_winters}')\n",
    "print('Last position of first and second year: {}, {}'.format(\n",
    "    last_year_pos[0], last_year_pos[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get first and last position of each winter for each year. A winter stretches from Dec-April -->only for entire winters (not like 1979 and 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_pos_winters = give_last_post_winters(last_year_pos, years, df_time)\n",
    "first_pos_winters = give_first_post_winters(last_year_pos, years, df_time)\n",
    "assert (len(last_pos_winters) == len(first_pos_winters))\n",
    "\n",
    "np.save(PATH1 + 'last_pos_winters.npy', last_pos_winters)\n",
    "np.save(PATH1 + 'first_pos_winters.npy', first_pos_winters)\n",
    "\n",
    "print('First position of first and second winter: {}, {}'.format(\n",
    "    first_pos_winters[0], first_pos_winters[1]))\n",
    "print('Last position of first and second winter: {}, {}'.format(\n",
    "    last_pos_winters[0], last_pos_winters[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first and last position of each winter, get the indices of all points considered during the Takens embedding. We range from the $first$ to the $last - \\tau$, where $\\tau$ is the size of the embedding (here two months). These indices will be the only ones considered during the building of the Takens matrix. Because $Y = X[1 : (T -\\tau); 1 : (Dx\\tau)]$ we stop at $last - \\tau$ when building the matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_of_points = []\n",
    "for i in range(len(first_pos_winters)):\n",
    "    indices_of_points.append(\n",
    "        range(first_pos_winters[i] , last_pos_winters[i] -TAU+1, 1))\n",
    "for i in indices_of_points:\n",
    "    # check that we take only full winters\n",
    "    assert (i[-1] - i[0] > 600)\n",
    "print('First three takens indicices:')\n",
    "indices_of_points[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Safety check to see everything is working all right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of points:\n",
    "le = np.sum([len(i) for i in indices_of_points])\n",
    "# time series corresponding to those points:\n",
    "time_nlsa = time[indices_of_points[0]]\n",
    "for i in range(1, len(last_pos_winters)):\n",
    "    time_nlsa = pd.concat([time_nlsa, time[indices_of_points[i]]], axis=0)\n",
    "assert (le == len(time_nlsa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unravel the indices from the ranges above into one list:\n",
    "indices_m = []\n",
    "for j in range(len(indices_of_points)):\n",
    "    for i in range(len(indices_of_points[j])):\n",
    "        indices_m.append(indices_of_points[j][i])\n",
    "assert(len(indices_m)==le)\n",
    "print(f'First and last indice: {indices_m[0]}, {indices_m[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the hull of the Takens matrix we're going to fill below. It's of shape $(T - m*\\tau, D*\\tau)$ where $m$ is the number of entire winters, $T$ the number of time-samples and $D$ is the number of samples we choose from the dimension of the original data matrix. Here we choose to take $D = 150$ while the original $D = 1003$. This is to fasten computations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of entire winters:\n",
    "num_years = END_YEAR - BEGIN_YEAR - 1\n",
    "print(f'Number of entire winters: {num_years}')\n",
    "\n",
    "X = df.drop(['Unnamed: 0', 'Date'], axis=1).values\n",
    "# Shape (T - m*Tau, D*Tau)\n",
    "D = 150\n",
    "print(f'Tau: {TAU}')\n",
    "print(f'D: {D}')\n",
    "m, n = le, TAU * D\n",
    "takens_Y = np.zeros((m, n))\n",
    "print(f'New Takens matrix shape: {takens_Y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the matrix by concatenating rows of the data matrix. Careful, don't concatenate with points of other years, that's why we extracted the indices of points before. This way we don't cross-over to another winter. \n",
    "\n",
    "Re-run this or load pre-saved matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%time\n",
    "m, n = takens_Y.shape[0], takens_Y.shape[1]\n",
    "print(f'Takens matrix being constructed of shape {takens_Y.shape}')\n",
    "\n",
    "for i in tqdm(range(m)):\n",
    "    indice_row = indices_m[i]\n",
    "    # X[t, 1:D]\n",
    "    row = X[indice_row, :D]\n",
    "    for t in range(1, TAU):\n",
    "        # X[t+1, 1:D]\n",
    "        row = np.concatenate((row, X[indice_row + t, :D]))\n",
    "    takens_Y[i, :len(row)] = row\n",
    "\n",
    "takens_Y_df = pd.DataFrame(takens_Y)\n",
    "\n",
    "# Save matrix:\n",
    "print(f'Saving Takens matrix at {PATH1}')\n",
    "takens_Y_df.to_pickle(PATH1 + 'takens_Y_df_{}.pkl'.format(D))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "takens_Y_df = pd.read_pickle(PATH1 + 'takens_Y_df_{}.pkl'.format(D))\n",
    "print(takens_Y_df.shape)\n",
    "takens_Y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(takens_Y_df.values)\n",
    "del takens_Y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create distance matrix:\n",
    "\n",
    "From this Takens matrix calculate the distance between all points. This will be used later when creating the Laplacian. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "D = 150\n",
    "\n",
    "if DATA == 'raw':\n",
    "    df = pd.read_csv(INPUT_DATA + 'raw_data_coefficients.csv', sep=',')\n",
    "elif DATA == 'anomalies': \n",
    "    df = pd.read_csv(INPUT_DATA + 'anomalies_coefficients.csv', sep=',')\n",
    "print(f'Input data shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: takes a lot of time to compute. Uncomment below if you want to re-compute it. Otherwise can just load a pre-saved one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%time\n",
    "X = df.drop(['Unnamed: 0', 'Date'], axis = 1).values[:,:D]\n",
    "\n",
    "print(f'Distance matrix for {DATA}')\n",
    "distance_df = pd.DataFrame(distance_matrix(X, X),\n",
    "                           index= df.drop(['Unnamed: 0', 'Date'], axis = 1).index,\n",
    "                           columns= df.drop(['Unnamed: 0', 'Date'], axis = 1).index)\n",
    "distance_df.to_pickle(PATH1 + 'distance_matrix_{}.pkl'.format(D))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f'Distance matrix read from {PATH1}')\n",
    "distance_matrix = pd.read_pickle(PATH1 + 'distance_matrix_{}.pkl'.format(D)).values\n",
    "\n",
    "#check distance matrix is symmetric:\n",
    "assert(np.all(distance_matrix.T == distance_matrix))\n",
    "\n",
    "print(f'Distance matrix shape: {distance_matrix.shape}')\n",
    "print('Sample of distance matrix:')\n",
    "pd.DataFrame(distance_matrix).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taken's embedding through distance matrix:\n",
    "\n",
    "A second method to get the distance matrix of the Takens embedding is to apply another formula:  \n",
    "\n",
    "$dist(Y(t_1), Y(t_2))=\\sqrt(\\sum_{j=0}^{\\tau-1}dist(X(t_1 +j),X(t_2+j))^2)$ \n",
    "\n",
    "Where $\\tau$ is the size of the embedding. Here we build the distance matrix directly from the distance matrix on the original data, without needing to build the Takens matrix first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read the original distance matrix:\n",
    "D = 150\n",
    "distance_matrix = pd.read_pickle(PATH1 + 'distance_matrix_{}.pkl'.format(D)).values\n",
    "print(f'Distance matrix shape: {distance_matrix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the original data:\n",
    "if DATA == 'raw':\n",
    "    df = pd.read_csv(INPUT_DATA + 'raw_data_coefficients.csv', sep=',')\n",
    "elif DATA == 'anomalies': \n",
    "    df = pd.read_csv(INPUT_DATA + 'anomalies_coefficients.csv', sep=',')\n",
    "print(f'Input data shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get time information and the number of measures per year: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe with time as index and remove useless columns\n",
    "time = df['Date']\n",
    "time = pd.to_datetime(time)\n",
    "df_ = df.drop(['Unnamed: 0', 'Date'], axis=1)\n",
    "df_time = pd.concat([time, df_], axis=1)\n",
    "df_time = df_time.set_index('Date')\n",
    "\n",
    "# Years in the data:\n",
    "years = range(BEGIN_YEAR, END_YEAR)\n",
    "years = [str(y) for y in years]\n",
    "\n",
    "# Count number of samples per year:\n",
    "counts = {}\n",
    "sum_ = 0\n",
    "for y in years:\n",
    "    sum_ += len(df_time[y])\n",
    "    counts[y] = len(df_time[y])\n",
    "print('For example, number of samples in 1979 and 1980: {}, {}'.format(counts['1979'], counts['1980']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the position of the last element for each year \n",
    "cs = [counts[i] for i in years]\n",
    "last_year_pos = [np.sum(cs[:i + 1]) for i in range(len(cs))]\n",
    "\n",
    "# get the number of entire winters e.g, winters that range \n",
    "# from Dec - April (not like 1979 and 2019)\n",
    "entire_winters = END_YEAR - BEGIN_YEAR - 1\n",
    "print(f'Number of entire winters in data: {entire_winters}')\n",
    "print('Last position of first and second year: {}, {}'.format(\n",
    "    last_year_pos[0], last_year_pos[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get first and last position of each winter for each year. A winter stretches from Dec-April -->only for entire winters (not like 1979 and 2019).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_pos_winters = give_last_post_winters(last_year_pos, years, df_time)\n",
    "first_pos_winters = give_first_post_winters(last_year_pos, years, df_time)\n",
    "assert (len(last_pos_winters) == len(first_pos_winters))\n",
    "\n",
    "np.save(PATH1 + 'last_pos_winters.npy', last_pos_winters)\n",
    "np.save(PATH1 + 'first_pos_winters.npy', first_pos_winters)\n",
    "\n",
    "print('First position of first and second winter: {}, {}'.format(\n",
    "    first_pos_winters[0], first_pos_winters[1]))\n",
    "print('Last position of first and second winter: {}, {}'.format(\n",
    "    last_pos_winters[0], last_pos_winters[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we build from the past i.e., from one point $i$ look back till $i-\\tau$ we need to keep only the indices of points that have at least $\\tau$ points behind them in their year. So go from $first+\\tau$ to $last$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_of_points = []\n",
    "for i in range(len(first_pos_winters)):\n",
    "    indices_of_points.append(\n",
    "        range(first_pos_winters[i] + TAU, last_pos_winters[i] + 1, 1))\n",
    "for i in indices_of_points:\n",
    "    assert (i[-1] - i[0] > 600)\n",
    "print('Range of first three winters:')\n",
    "indices_of_points[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Safety check to see everything is working all right:\n",
    "# total number of points:\n",
    "le = np.sum([len(i) for i in indices_of_points])\n",
    "# time series corresponding to those points:\n",
    "time_nlsa = time[indices_of_points[0]]\n",
    "for i in range(1, len(last_pos_winters)):\n",
    "    time_nlsa = pd.concat([time_nlsa, time[indices_of_points[i]]], axis=0)\n",
    "assert (le == len(time_nlsa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unwrap the indices of all ranges of `indices_of_points` to get one list of all indices we need to go over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_m = []\n",
    "for j in range(len(indices_of_points)):\n",
    "    for i in range(len(indices_of_points[j])):\n",
    "        indices_m.append(indices_of_points[j][i])\n",
    "assert (len(indices_m) == le)\n",
    "print(f'First and last indice: {indices_m[0]}, {indices_m[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the new Takens distance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of entire winters:\n",
    "num_years = END_YEAR-BEGIN_YEAR-1\n",
    "print(f'Number of entire winters: {num_years}')\n",
    "m,n  = le, le\n",
    "dist_Y = np.zeros((m,n))\n",
    "print(f'New distance matrix shape: {dist_Y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment below to recompute it or just load a pre-saved one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%time\n",
    "dist_Y = apply_takens(dist_Y, distance_matrix, indices_m, TAU, PATH1)\n",
    "\n",
    "#print(f'Reading upper triangle matrix at {PATH1}:')\n",
    "#dist_Y = pd.read_pickle(PATH1+'dist_Y_takens_final.pkl').values\n",
    "\n",
    "# Make matrix symetric:\n",
    "takens_matrix = dist_Y + dist_Y.T\n",
    "\n",
    "# Save NLSA distance matrix matrix:\n",
    "print(f'Save NLSA distance matrix at {PATH1}')\n",
    "takens_df = pd.DataFrame(takens_matrix)\n",
    "takens_df.to_pickle(PATH1 + 'distance_matrix_takens_final.pkl')\n",
    "\n",
    "del dist_Y\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read NLSA distance matrix from memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Reading Takens matrix:')\n",
    "D = 150\n",
    "takens_matrix = pd.read_pickle(\n",
    "    PATH1 + 'distance_matrix_takens_{}.pkl'.format(D)).values\n",
    "\n",
    "#check distance matrix is symmetric:\n",
    "assert (np.all(takens_matrix.T == takens_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(takens_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(takens_matrix)\n",
    "del takens_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbours:\n",
    "Calculate matrix with neighbouring vertices, e.g. 1 for $x_j,x_i$ if $x_j$ or $x_i$ selected the other as a closest  neighbour and 0 otherwise. This weight matrix will also serve as the simple kernel. Creates the binary weight matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Percentage of neighbours\n",
    "perc = PERC_NEIGH / 100\n",
    "D = 150\n",
    "print(f'Starting nearest neighbours for {perc*100}% nearest neihbours ')\n",
    "\n",
    "# Reading input data:\n",
    "if DATA == 'raw':\n",
    "    print(f'Reading raw input data from: {INPUT_DATA}')\n",
    "    df = pd.read_csv(INPUT_DATA + 'raw_data_coefficients.csv', sep=',')\n",
    "elif DATA == 'anomalies': \n",
    "    print(f'Reading anomalies input data from: {INPUT_DATA}')\n",
    "    df = pd.read_csv(INPUT_DATA + 'anomalies_coefficients.csv', sep=',')\n",
    "\n",
    "print(f'Reading distance matrix')\n",
    "\n",
    "if USE_TAKENS == True:\n",
    "    print(f'Using takens embedding, reading from {PATH1}')\n",
    "    distance_matrix = pd.read_pickle(PATH1 + 'distance_matrix_takens_{}.pkl'.format(D)).values\n",
    "    print(f'Distance matrix shape: {distance_matrix.shape}')\n",
    "else:\n",
    "    print(f'Using normal distance matrix, reading from {PATH1}')\n",
    "    distance_matrix = pd.read_pickle(PATH1+'distance_matrix_{}.pkl'.format(D)).values\n",
    "    print(f'Distance matrix shape: {distance_matrix.shape}')\n",
    "\n",
    "# K-nearest neighbours:\n",
    "print(f'Look for {perc*100}% nearest neihbours:')\n",
    "K = int(len(df.values) * perc)\n",
    "N = len(distance_matrix)\n",
    "weight_matrix = np.zeros((N, N))\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    # select K closest neihbours:\n",
    "    indices = np.argsort(distance_matrix[i])[:K]\n",
    "    for j in indices:\n",
    "        if i != j and weight_matrix[i, j] == 0:\n",
    "            weight_matrix[i, j] += 1\n",
    "weight_matrix = weight_matrix.T\n",
    "\n",
    "# Make weight matrix is symmetric:\n",
    "for i in tqdm(range(N)):\n",
    "    indices = np.argsort(distance_matrix.T[i])[:K]\n",
    "    for j in indices:\n",
    "        if i != j and weight_matrix[i, j] == 0:\n",
    "            weight_matrix[i, j] += 1\n",
    "weight_df = pd.DataFrame(weight_matrix)\n",
    "\n",
    "# Save weight matrix:\n",
    "if USE_TAKENS:\n",
    "    PATH_SAVE = PATH_SIMPLE_TAKENS+'weight_matrix_takens_{}.pkl'.format(PERC_NEIGH)\n",
    "    print('Saving weight matrix at {}'.format(PATH_SAVE))\n",
    "    weight_df.to_pickle(PATH_SAVE)\n",
    "else:\n",
    "    PATH_SAVE = PATH_SIMPLE+'weight_matrix_{}.pkl'.format(PERC_NEIGH)\n",
    "    print('Saving weight matrix at {}'.format(PATH_SAVE))\n",
    "    weight_df.to_pickle(PATH_SAVE)\n",
    "\n",
    "del weight_df, weight_matrix, distance_matrix\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read weight matrix:\n",
    "if USE_TAKENS:\n",
    "    print(f'Reading binary weight matrix from {PATH_SIMPLE_TAKENS}')\n",
    "    weight_m = pd.read_pickle(PATH_SIMPLE_TAKENS +\n",
    "                          'weight_matrix_takens_{}.pkl'.format(PERC_NEIGH)).values\n",
    "else:\n",
    "    print(f'Reading binary weight matrix from {PATH_SIMPLE}')\n",
    "    weight_m = pd.read_pickle(PATH_SIMPLE +\n",
    "                          'weight_matrix_{}.pkl'.format(PERC_NEIGH)).values\n",
    "# Test if matrix is symetric:\n",
    "assert (np.all(weight_m.T == weight_m))\n",
    "print(f'Weight matrix shape: {weight_m.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(weight_m).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at image of weight matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(weight_m)\n",
    "del weight_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary kernel: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Laplacian eigenmaps for the binary kernel (0/1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagonal matrix:\n",
    "Diagonal matrix from binary weight matrix, computed as $D_{ij} = \\sum_{j}W_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if USE_TAKENS:\n",
    "    weight_m = pd.read_pickle(PATH_SIMPLE_TAKENS +\n",
    "                          'weight_matrix_takens_{}.pkl'.format(PERC_NEIGH)).values\n",
    "else:\n",
    "    weight_m = pd.read_pickle(PATH_SIMPLE +\n",
    "                          'weight_matrix_{}.pkl'.format(PERC_NEIGH)).values\n",
    "\n",
    "# Compute diagonal matrix:\n",
    "D = np.zeros((len(weight_m), len(weight_m)))\n",
    "print('Computing diagonal matrix:')\n",
    "for i in tqdm(range(len(weight_m))):\n",
    "    D[i, i] = np.sum(weight_m[i])\n",
    "D_df = pd.DataFrame(D)\n",
    "\n",
    "\n",
    "if USE_TAKENS:\n",
    "    print(f'Saving diagonal matrix at: {PATH_SIMPLE_TAKENS}')\n",
    "    D_df.to_pickle(PATH_SIMPLE_TAKENS+'diagonal_matrix_takens_{}.pkl'.format(PERC_NEIGH))\n",
    "else:\n",
    "    print(f'Saving diagonal matrix at: {PATH_SIMPLE}')\n",
    "    D_df.to_pickle(PATH_SIMPLE+'diagonal_matrix_{}.pkl'.format(PERC_NEIGH))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplacian matrix:\n",
    "\n",
    "Laplacian matrix computed as $L = D-W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Computing Laplacian matrix:')\n",
    "\n",
    "L = np.subtract(D, weight_m)\n",
    "L_df = pd.DataFrame(L)\n",
    "\n",
    "if USE_TAKENS: \n",
    "    print(f'Saving laplacian matrix at: {PATH_SIMPLE_TAKENS}')\n",
    "    L_df.to_pickle(PATH_SIMPLE_TAKENS + 'laplacian_simple_matrix_takens_{}.pkl'.format(PERC_NEIGH))\n",
    "else:\n",
    "    print(f'Saving laplacian matrix at: {PATH_SIMPLE}')\n",
    "    L_df.to_pickle(PATH_SIMPLE + 'laplacian_simple_matrix_{}.pkl'.format(PERC_NEIGH))\n",
    "\n",
    "del L, L_df, D, weight_m\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvalues:\n",
    "Eigenvalues and eigenvectors for binary kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if USE_TAKENS:\n",
    "    print(f'Reading D and L from: {PATH_SIMPLE_TAKENS}')\n",
    "    D = pd.read_pickle(\n",
    "        PATH_SIMPLE_TAKENS +\n",
    "        'diagonal_matrix_takens_{}.pkl'.format(PERC_NEIGH)).values\n",
    "    print(f'Diagonal matrix shape: {D.shape}')\n",
    "    L = pd.read_pickle(\n",
    "        PATH_SIMPLE_TAKENS +\n",
    "        'laplacian_simple_matrix_takens_{}.pkl'.format(PERC_NEIGH)).values\n",
    "    print(f'Laplacian matrix shape: {L.shape}')\n",
    "\n",
    "else:\n",
    "    print(f'Reading D and L from: {PATH_SIMPLE}')\n",
    "    D = pd.read_pickle(PATH_SIMPLE +\n",
    "                       'diagonal_matrix_{}.pkl'.format(PERC_NEIGH)).values\n",
    "    L = pd.read_pickle(\n",
    "        PATH_SIMPLE +\n",
    "        'laplacian_simple_matrix_{}.pkl'.format(PERC_NEIGH)).values\n",
    "\n",
    "# Compute first eigenvalues\n",
    "print(f'Computing {NUM_EIGENVALUES} eigenvalues:')\n",
    "w, eigv = linalg.eigs(L, k=NUM_EIGENVALUES, M=D, which='SM')\n",
    "\n",
    "# Save values:\n",
    "if USE_TAKENS:\n",
    "    print(f'Saving eigenvalues and eigenvectors to: {PATH_SIMPLE_TAKENS}')\n",
    "    pd.DataFrame(w).to_pickle(PATH_SIMPLE_TAKENS +\n",
    "                              'eigenvalues_takens_{}.pkl'.format(PERC_NEIGH))\n",
    "    pd.DataFrame(eigv).to_pickle(\n",
    "        PATH_SIMPLE_TAKENS + 'eigenvectors_takens_{}.pkl'.format(PERC_NEIGH))\n",
    "    print(f'Eigenvector shape: {eigv.shape}')\n",
    "else:\n",
    "    print(f'Saving eigenvalues and eigenvectors to: {PATH_SIMPLE}')\n",
    "    pd.DataFrame(w).to_pickle(PATH_SIMPLE +\n",
    "                              'eigenvalues_{}.pkl'.format(PERC_NEIGH))\n",
    "    pd.DataFrame(eigv).to_pickle(PATH_SIMPLE +\n",
    "                                 'eigenvectors_{}.pkl'.format(PERC_NEIGH))\n",
    "del L, D, w, eigv\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat kernel: \n",
    "The heat kernel is computed as $W_{ij} = exp(\\frac{-||x_j-x_i||^2_2}{t})$ for connected neighbours and 0 otherwise.\n",
    "Source:[paper](https://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of bandwidth: \n",
    "Compute the right bandwidth $t$ values for the heat kernel. Try the mean, median and maximum over distances of neihgbouring vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f'Reading distance and weight matrix from: {PATH1}')\n",
    "\n",
    "if USE_TAKENS: \n",
    "    print('Using takens:')\n",
    "    distance_matrix = pd.read_pickle(PATH1 + 'distance_matrix_takens.pkl').values\n",
    "    weight_m = pd.read_pickle(PATH_SIMPLE_TAKENS +'weight_matrix_takens_{}.pkl'.format(PERC_NEIGH)).values\n",
    "else:\n",
    "    distance_matrix = pd.read_pickle(PATH1+'distance_matrix.pkl').values\n",
    "    weight_m = pd.read_pickle(PATH_SIMPLE +'weight_matrix_{}.pkl'.format(PERC_NEIGH)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Select only distances that are chosen in the nearest neighbours step:\n",
    "mult_df = np.multiply(distance_matrix, weight_m)\n",
    "non_zero_mult = np.extract(mult_df > 0, mult_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean, median and max over non-zero distances:\n",
    "mean_distances = np.mean(non_zero_mult)\n",
    "median_dist = np.median(non_zero_mult)\n",
    "max_distances = np.max(non_zero_mult)\n",
    "\n",
    "t = [mean_distances, median_dist, max_distances]\n",
    "\n",
    "ts = {\n",
    "    'mean_distances': mean_distances,\n",
    "    'median_dist': median_dist,\n",
    "    'max_distances': max_distances,\n",
    "    'mean_dist_all': np.mean(distance_matrix),\n",
    "    'max_dist_all': np.max(distance_matrix),\n",
    "}\n",
    "if USE_TAKENS:\n",
    "    np.save(PATH1 + 't_takens_.npy', np.array(t))\n",
    "else:\n",
    "    np.save(PATH1+'t.npy', np.array(t))\n",
    "print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,5))\n",
    "axs = plt.subplot(1,1,1)\n",
    "axs.hist(non_zero_mult)\n",
    "#plt.title('Non zero distances between neighbours:')\n",
    "axs.axvline(t[0], color = 'green', label = 'mean')\n",
    "axs.axvline(t[1], color = 'orange', label = 'median')\n",
    "axs.axvline(t[2], color = 'red', label = 'max')\n",
    "axs.set_xlabel('Distance between neighbours')\n",
    "axs.set_ylabel('Count')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del distance_matrix, weight_m, mult_df, non_zero_mult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heat matrix: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing heat kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if USE_TAKENS:\n",
    "    t = np.load(PATH1+'t_takens_.npy')\n",
    "    chosen_t = t[0]\n",
    "    corr_t = ['mean']\n",
    "    \n",
    "    print(f'Reading distance and weight matrix from: {PATH_SIMPLE}')\n",
    "    distance_matrix = pd.read_pickle(PATH1 + 'distance_matrix_takens.pkl').values\n",
    "    weight_m = pd.read_pickle(PATH_SIMPLE_TAKENS+'weight_matrix_takens_{}.pkl'.format(PERC_NEIGH)).values\n",
    "    \n",
    "    print('Computing heat matrix for bandwitdh {}: {}'.format(corr_t[0],chosen_t))\n",
    "    PATH2 = PATH1+'t_'+corr_t[0]+'/takens/'\n",
    "    if not os.path.exists(PATH2):\n",
    "            os.makedirs(PATH2)\n",
    "\n",
    "    distance_df = pd.DataFrame(distance_matrix)\n",
    "    # Create heat matrix:\n",
    "    heat_matrix_df = distance_df.apply(lambda x: np.exp(-(x**2) / (chosen_t**2)))\n",
    "    heat_matrix_df = pd.DataFrame(np.multiply(weight_m, heat_matrix_df))\n",
    "    print(f'Saving heat matrix at {PATH2}')\n",
    "    heat_matrix_df.to_pickle(PATH2+'heat_matrix_'+'t_'+corr_t[0]+ '_takens_.pkl')\n",
    "    \n",
    "else:\n",
    "    t = np.load(PATH1+'t.npy')\n",
    "    corr_t = ['mean', 'med', 'max']\n",
    "    \n",
    "    print(f'Reading distance and weight matrix from: {PATH1}')\n",
    "    distance_matrix = pd.read_pickle(PATH1+'distance_matrix.pkl').values\n",
    "    weight_m = pd.read_pickle(PATH_SIMPLE+'weight_matrix_{}.pkl'.format(PERC_NEIGH)).values\n",
    "    \n",
    "    for i in range(3):\n",
    "        chosen_t = t[i]\n",
    "        print('Computing heat matrix for bandwitdh {}: {}'.format(corr_t[i],chosen_t))\n",
    "\n",
    "        PATH2 = PATH1+'t_'+corr_t[i]+'/'\n",
    "        if not os.path.exists(PATH2):\n",
    "            os.makedirs(PATH2)\n",
    "\n",
    "        distance_df = pd.DataFrame(distance_matrix)\n",
    "        # Create heat matrix:\n",
    "        heat_matrix_df = distance_df.apply(lambda x: np.exp(-(x**2) / (chosen_t**2)))\n",
    "        heat_matrix_df = pd.DataFrame(np.multiply(weight_m, heat_matrix_df))\n",
    "        heat_matrix_df.to_pickle(PATH2+'heat_matrix_'+'t_'+corr_t[i]+ '_.pkl')\n",
    "del heat_matrix_df, distance_matrix, weight_m, distance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-computed heat kernels for 10% neighbours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corr_t = ['mean', 'med', 'max']\n",
    "\n",
    "\n",
    "if USE_TAKENS:\n",
    "    PATH_READ = '../../../data/vandermeer/pickles/{}/10perc/'.format(DATA)\n",
    "    print(f'Path to heat matrices:{PATH_READ}')\n",
    "    print(f'Reading heat matrix for t = {corr_t[0]}:')\n",
    "    heat_matrix_mean = pd.read_pickle(PATH_READ +'t_'+corr_t[0]+ '/takens/' +\n",
    "                                      'heat_matrix_' +'t_'+corr_t[0]+'_takens_.pkl').values\n",
    "    print(f'Heat matrix shape: {heat_matrix_mean.shape}')\n",
    "else:\n",
    "    PATH_READ = '../../../data/vandermeer/pickles/{}/10perc/'.format(DATA)\n",
    "    print(f'Path to heat matrices:{PATH_READ}')\n",
    "    print(f'Reading heat matrix for t = {corr_t[0]}:')\n",
    "    heat_matrix_mean = pd.read_pickle(PATH_READ +'t_'+corr_t[0]+ '/' +\n",
    "                                      'heat_matrix_' +'t_'+corr_t[0]+'_.pkl').values\n",
    "    print(f'Reading heat matrix for t = {corr_t[1]}:')\n",
    "    heat_matrix_max = pd.read_pickle(PATH_READ +'t_'+corr_t[1]+ '/' +\n",
    "                                     'heat_matrix_' +'t_'+corr_t[1]+'_.pkl').values\n",
    "    print(f'Reading heat matrix for t = {corr_t[2]}:')\n",
    "    heat_matrix_med = pd.read_pickle(PATH_READ +'t_'+corr_t[2]+ '/' +\n",
    "                                     'heat_matrix_' +'t_'+corr_t[2]+'_.pkl').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-computed heat kernels for 20% neighbours if you want to compare to those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not USE_TAKENS:\n",
    "    corr_t = ['mean', 'med', 'max']\n",
    "    PATH_READ = '../../../data/vandermeer/pickles/{}/20perc/'.format(DATA)\n",
    "    print(f'Path to heat matrices:{PATH_READ}')\n",
    "    print(f'Reading heat matrix for t = {corr_t[0]}:')\n",
    "    heat_matrix_mean2 = pd.read_pickle(PATH_READ +'t_'+corr_t[0]+ '/' +\n",
    "                                      'heat_matrix_' +'t_'+corr_t[0]+\n",
    "                                      '_.pkl').values\n",
    "    print(f'Reading heat matrix for t = {corr_t[1]}:')\n",
    "    heat_matrix_max2 = pd.read_pickle(PATH_READ +'t_'+corr_t[1]+ '/' +\n",
    "                                     'heat_matrix_' +'t_'+corr_t[1]+\n",
    "                                     '_.pkl').values\n",
    "    print(f'Reading heat matrix for t = {corr_t[2]}:')\n",
    "    heat_matrix_med2 = pd.read_pickle(PATH_READ +'t_'+corr_t[2]+ '/' +\n",
    "                                     'heat_matrix_' +'t_'+corr_t[2]+\n",
    "                                     '_.pkl').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot sample of heat kernels as image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not USE_TAKENS:\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 10))\n",
    "    i = 0\n",
    "    corr_t = ['mean', 'med', 'max']\n",
    "    matrices = [heat_matrix_mean[:10, :10],heat_matrix_max[:10, :10],heat_matrix_med[:10, :10]]\n",
    "    for chosen_t in corr_t:\n",
    "        axs[i].imshow(matrices[i])\n",
    "        axs[i].set_title(chosen_t)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histogram of non zero values of heat kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if USE_TAKENS:\n",
    "    fig, axs = plt.subplots(1,1, figsize = (5,5))\n",
    "    corr_t = ['mean']\n",
    "    non_zero_heat = np.extract(heat_matrix_mean>0, heat_matrix_mean)\n",
    "    axs.set_title('Heat kernel, t = {} distance'.format(corr_t[0]))\n",
    "    axs.hist(non_zero_heat, label = '10 perc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if not USE_TAKENS:\n",
    "    fig, axs = plt.subplots(1,3, figsize = (15,5))\n",
    "    i = 0\n",
    "    corr_t = ['mean', 'med', 'max']\n",
    "    matrices = [heat_matrix_mean, heat_matrix_max, heat_matrix_med]\n",
    "    for i in range(3):\n",
    "        heat_matrix = matrices[i]\n",
    "        non_zero_heat = np.extract(heat_matrix>0, heat_matrix)\n",
    "        axs[i].set_title('Heat kernel, t = {} distance'.format(corr_t[i]))\n",
    "        axs[i].hist(non_zero_heat, label = '10 perc')\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot overlapping histogram of non zero values of heat kernels for 10% and 20%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not USE_TAKENS:\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    i = 0\n",
    "    matrices = [heat_matrix_mean, heat_matrix_max, heat_matrix_med]\n",
    "    matrices2 = [heat_matrix_mean2, heat_matrix_max2, heat_matrix_med2]\n",
    "    corr_t = ['mean', 'med', 'max']\n",
    "    for i in range(3):\n",
    "        heat_matrix = matrices[i]\n",
    "        heat_matrix2 = matrices2[i]\n",
    "        non_zero_heat = np.extract(heat_matrix > 0, heat_matrix)\n",
    "        non_zero_heat2 = np.extract(heat_matrix2 > 0, heat_matrix2)\n",
    "        t = ['mean', 'max', 'median']\n",
    "        axs[i].set_title('Heat kernel, t = {} distance'.format(corr_t[i]))\n",
    "        axs[i].hist(non_zero_heat, label='10 perc', alpha=0.5)\n",
    "        axs[i].hist(non_zero_heat2, label='20 perc', alpha=0.5)\n",
    "        i += 1\n",
    "    plt.legend()\n",
    "    del matrices, matrices2, heat_matrix_mean, heat_matrix_max, heat_matrix_med, heat_matrix_mean2, heat_matrix_max2, heat_matrix_med2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagonal weight matrix: \n",
    "Compute diagonal matrix as $D_{ii} = \\sum_j W_{ij}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TAKENS:\n",
    "    corr_t = ['mean']\n",
    "    print(f'Computing diagonal matrix for heat kernel with t: {corr_t[0]}')\n",
    "    PATH2 = PATH1+'t_'+corr_t[0]+'/takens/'\n",
    "    if not os.path.exists(PATH2):\n",
    "        os.makedirs(PATH2)\n",
    "    wm = pd.read_pickle(PATH1+'t_'+corr_t[0]+'/takens/heat_matrix_t_'+corr_t[0]+'_takens_.pkl').values\n",
    "    D = np.zeros((len(wm),len(wm)))\n",
    "    for i in tqdm(range(len(wm))):\n",
    "        D[i,i] = np.sum(wm[i])\n",
    "    D_df = pd.DataFrame(D)\n",
    "    print(f'Writing diagonal matrix to {PATH2}')\n",
    "    D_df.to_pickle(PATH2+'diagonal_heat_matrix_t_'+corr_t[0]+'_takens_.pkl')\n",
    "else:\n",
    "    corr_t = ['mean', 'med', 'max']\n",
    "    for j in range(3):\n",
    "        print(f'Computing diagonal matrix for heat kernel with t: {corr_t[j]}')\n",
    "        PATH2 = PATH1+'t_'+corr_t[j]+'/'\n",
    "        if not os.path.exists(PATH2):\n",
    "            os.makedirs(PATH2)\n",
    "        wm = pd.read_pickle(PATH1+'t_'+corr_t[j]+'/heat_matrix_t_'+corr_t[j]+'_.pkl').values\n",
    "        D = np.zeros((len(wm),len(wm)))\n",
    "        for i in tqdm(range(len(wm))):\n",
    "            D[i,i] = np.sum(wm[i])\n",
    "        D_df = pd.DataFrame(D)\n",
    "        print(f'Writing diagonal matrix to {PATH2}')\n",
    "        D_df.to_pickle(PATH2+'diagonal_heat_matrix_t_'+corr_t[j]+'_.pkl')\n",
    "del D_df, D, wm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-computed diagonal matrix_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if USE_TAKENS:\n",
    "    D = pd.read_pickle(PATH1+'t_mean/takens/diagonal_heat_matrix_'+'t_mean_takens_.pkl').values\n",
    "else:\n",
    "    D = pd.read_pickle(PATH1+'t_mean/diagonal_heat_matrix_'+'t_mean_.pkl').values\n",
    "print(f'Shape of diagonal matrix: {D.shape}')\n",
    "pd.DataFrame(D).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplacian matrix: \n",
    "\n",
    "Compute Laplacian matrix as $L = D- W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if USE_TAKENS:\n",
    "    corr_t = ['mean']\n",
    "    print(f'Computing Laplacian matrix for heat kernel with t: {corr_t[0]}')\n",
    "    PATH2 = PATH1 + 't_' + corr_t[0] + '/takens/'\n",
    "    if not os.path.exists(PATH2):\n",
    "        os.makedirs(PATH2)\n",
    "    print('Reading heat matrix:')\n",
    "    wm = pd.read_pickle(PATH2+'heat_matrix_t_'+corr_t[0]+'_takens_.pkl').values\n",
    "    print('Reading diagonal matrix:')\n",
    "    D = pd.read_pickle(PATH2+'diagonal_heat_matrix_t_'+corr_t[0]+'_takens_.pkl').values\n",
    "    print('Calculating Laplacian:')\n",
    "    L = np.subtract(D, wm)\n",
    "    L_df = pd.DataFrame(L)\n",
    "    print(f'Writing Laplacian to {PATH2}')\n",
    "    L_df.to_pickle(PATH2+'laplacian_heat_matrix_t_'+corr_t[0]+'_takens_.pkl')\n",
    "\n",
    "else:\n",
    "    corr_t = ['mean', 'med', 'max']\n",
    "    for j in range(3):\n",
    "        print(f'Computing Laplacian matrix for heat kernel with t: {corr_t[j]}')\n",
    "        PATH2 = PATH1 + 't_' + corr_t[j] + '/'\n",
    "        if not os.path.exists(PATH2):\n",
    "            os.makedirs(PATH2)\n",
    "        print('Reading heat matrix:')\n",
    "        wm = pd.read_pickle(PATH2+'heat_matrix_t_'+corr_t[j]+'_.pkl').values\n",
    "        print('Reading diagonal matrix:')\n",
    "        D = pd.read_pickle(PATH2+'diagonal_heat_matrix_t_'+corr_t[j]+'_.pkl').values\n",
    "        print('Calculating Laplacian:')\n",
    "        L = np.subtract(D, wm)\n",
    "        L_df = pd.DataFrame(L)\n",
    "        L_df.to_pickle(PATH2+'laplacian_heat_matrix_t_'+corr_t[j]+'_.pkl')\n",
    "    del L_df, L, D, wm\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-computed laplacian matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if USE_TAKENS:\n",
    "    D = pd.read_pickle(PATH1+'t_mean/takens/laplacian_heat_matrix_t_mean_takens_.pkl').values\n",
    "else:\n",
    "    D = pd.read_pickle(PATH1+'t_mean/laplacian_heat_matrix_t_mean_.pkl').values\n",
    "print(f'Shape of Laplacian matrix: {L.shape}')\n",
    "pd.DataFrame(L).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvalues: \n",
    "\n",
    "Eigendecomposition of: $Lf = \\gamma Df$ where $f$ are the eigenvector solutions ordered according to their increasing eigenvalue $\\lambda_0 = 0 < \\lambda_1 < ...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing eigendecomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chosen_t = 'mean'\n",
    "print(f'Computing eigenvalues for heat kernel with t={chosen_t}')\n",
    "\n",
    "if not os.path.exists(PATH2):os.makedirs(PATH2)\n",
    "\n",
    "if USE_TAKENS:  \n",
    "    PATH2 = PATH1+'t_{}/takens/'.format(chosen_t)\n",
    "    print(f'Loading L from:{PATH2}')   \n",
    "    L = pd.read_pickle(PATH2+'laplacian_heat_matrix_t_'+chosen_t+'_takens_.pkl').values\n",
    "\n",
    "    print(f'Loading D from:{PATH2}')   \n",
    "    D = pd.read_pickle(PATH2+'diagonal_heat_matrix_t_'+chosen_t+'_takens_.pkl').values\n",
    "\n",
    "    print(f'Computing {NUM_EIGENVALUES} eigenvalues:')\n",
    "    w, eigv = linalg.eigs(L, k=NUM_EIGENVALUES, M=D, which='SM')\n",
    "\n",
    "    print('Saving eigendecomposition:')\n",
    "    pd.DataFrame(w).to_pickle(PATH2 + 'eigenval_t_'+chosen_t+f'_takens_{150}.pkl')\n",
    "    pd.DataFrame(eigv).to_pickle(PATH2 + 'eigenvec_t_'+chosen_t+ f'_takens_{150}.pkl')\n",
    "    print(f'Shape of eigenvectors: {eigv.shape}')\n",
    "else:\n",
    "    PATH2 = PATH1+'t_{}/'.format(chosen_t)\n",
    "    print(f'Loading L from:{PATH2}')   \n",
    "    L = pd.read_pickle(PATH2+'laplacian_heat_matrix_t_'+chosen_t+'_.pkl').values\n",
    "\n",
    "    print(f'Loading D from:{PATH2}')   \n",
    "    D = pd.read_pickle(PATH2+'diagonal_heat_matrix_t_'+chosen_t+'_.pkl').values\n",
    "\n",
    "    print(f'Computing {NUM_EIGENVALUES} eigenvalues:')\n",
    "    w, eigv = linalg.eigs(L, k=NUM_EIGENVALUES, M=D, which='SM')\n",
    "\n",
    "    print('Saving eigendecomposition:')\n",
    "    pd.DataFrame(w).to_pickle(PATH2 + 'eigenvalues_heat_matrix_t_'+chosen_t+'_.pkl')\n",
    "    pd.DataFrame(eigv).to_pickle(PATH2 + 'eigenvectors_heat_matrix_t_'+chosen_t+ '_.pkl')\n",
    "\n",
    "del L, D, w, eigv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "229.422px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
